{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc CSV ban đầu và CSV mới\n",
    "#csv_1 = pd.read_csv('./OANDA_BTCUSD, 15 (9).csv')\n",
    "csv_2 = pd.read_csv('./data/OANDA_XAUUSDNEW_Historical.csv')\n",
    "csv_1 = pd.read_csv('./data/OANDA_XAUUSD_1m_Historical.csv')\n",
    "\n",
    "\n",
    "# Chèn nội dung của CSV mới vào CSV ban đầu\n",
    "\n",
    "csv_ban_dau = pd.concat([csv_2, csv_1], ignore_index=True)\n",
    "\n",
    "# Lưu lại vào file mới\n",
    "csv_ban_dau.to_csv('./data/OANDA_XAUUSD_1m_Historical.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc CSV ban đầu và CSV mới\n",
    "csv_1 = pd.read_csv('./OANDA_XAUUSD, 1.csv')\n",
    "csv_2 = pd.read_csv('./OANDA_XAUUSD, 1 (1).csv')\n",
    "csv_3 = pd.read_csv('./OANDA_XAUUSD, 1 (2).csv')\n",
    "csv_4 = pd.read_csv('./OANDA_XAUUSD, 1 (3).csv')\n",
    "csv_5 = pd.read_csv('./OANDA_XAUUSD, 1 (4).csv')\n",
    "csv_6 = pd.read_csv('./OANDA_XAUUSD, 1 (5).csv')\n",
    "csv_7 = pd.read_csv('./OANDA_XAUUSD, 1 (6).csv')\n",
    "csv_8 = pd.read_csv('./OANDA_XAUUSD, 1 (7).csv')\n",
    "csv_9 = pd.read_csv('./OANDA_XAUUSD, 1 (8).csv')\n",
    "csv_10 = pd.read_csv('./OANDA_XAUUSD, 1 (9).csv')\n",
    "csv_11 = pd.read_csv('./OANDA_XAUUSD, 1 (10).csv')\n",
    "csv_12 = pd.read_csv('./OANDA_XAUUSD, 1 (11).csv')\n",
    "csv_13 = pd.read_csv('./OANDA_XAUUSD, 1 (12).csv')\n",
    "csv_14 = pd.read_csv('./OANDA_XAUUSD, 1 (13).csv')\n",
    "csv_15 = pd.read_csv('./OANDA_XAUUSD, 1 (14).csv')\n",
    "csv_16 = pd.read_csv('./OANDA_XAUUSD, 1 (5).csv')\n",
    "csv_17 = pd.read_csv('./OANDA_XAUUSD, 1 (16).csv')\n",
    "csv_18 = pd.read_csv('./OANDA_XAUUSD, 1 (17).csv')\n",
    "csv_19 = pd.read_csv('./OANDA_XAUUSD, 1 (18).csv')\n",
    "\n",
    "# Chèn nội dung của CSV mới vào CSV ban đầu\n",
    "\n",
    "csv_ban_dau = pd.concat([\n",
    "\n",
    "    csv_19,\n",
    "    csv_18,\n",
    "    csv_17,\n",
    "    csv_16,\n",
    "    csv_15, \n",
    "    csv_14, \n",
    "    csv_13,\n",
    "    csv_12, \n",
    "    csv_11, \n",
    "    csv_10, \n",
    "    csv_9,\n",
    "    csv_8,\n",
    "    csv_7,\n",
    "    csv_6,\n",
    "    csv_5,\n",
    "    csv_4,\n",
    "    csv_3,\n",
    "    csv_2,\n",
    "    csv_1,\n",
    "    ], ignore_index=True\n",
    ")\n",
    "\n",
    "# Lưu lại vào file mới\n",
    "csv_ban_dau.to_csv('./data/OANDA_XAUUSDNEW_Historical.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Định nghĩa các cột cần sử dụng\n",
    "columns = [\"close\", \"open\", \"high\", \"low\"]\n",
    "\n",
    "# Định nghĩa các đường dẫn tới file dữ liệu\n",
    "file_paths = [\n",
    "    './train-data/OANDA_XAUUSD_Historical_Updated.csv',\n",
    "]\n",
    "\n",
    "# Khởi tạo scaler cho từng cột\n",
    "scalers = {}\n",
    "scalers['Price'] = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Tạo danh sách để lưu trữ dữ liệu từ tất cả các file\n",
    "all_data = []\n",
    "\n",
    "# Load và thu thập dữ liệu từ tất cả các file\n",
    "for file in file_paths:\n",
    "    data = pd.read_csv(file)\n",
    "    data = data[columns].dropna()\n",
    "    all_data.append(data)\n",
    "\n",
    "# Kết hợp tất cả dữ liệu lại\n",
    "combined_data = pd.concat(all_data, axis=0)\n",
    "\n",
    "df_melted = combined_data.melt(var_name='Type', value_name='Value')\n",
    "\n",
    "scalers['Price'].fit(df_melted['Value'].values.reshape(-1, 1))\n",
    "    \n",
    "\n",
    "# Lưu các scaler vào file\n",
    "with open('./scalers/scalers_1.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "print(\"Scalers have been fitted and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "\n",
    "\n",
    "columns = [\"body\", \"STOCH_D\", \"STOCH_K\"]\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('./train-data/OANDA_XAUUSD_Historical.csv')\n",
    "features = data[columns].values\n",
    "scaled_data = np.zeros_like(features).astype(float)\n",
    "# Initialize and fit scalers\n",
    "scalers = {}\n",
    "# Scale the features\n",
    "for i, column in enumerate(columns):\n",
    "    scalers[column] = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data[:, i] = scalers[column].fit_transform(features[:, i].reshape(-1, 1)).flatten()\n",
    "# Create the sliding window dataset\n",
    "def create_multifeature_dataset(dataset, time_step=60):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        Y.append(dataset[i + time_step, 0])  \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 20\n",
    "X, y = create_multifeature_dataset(scaled_data, time_step)\n",
    "\n",
    "# Adjust input shape to include features\n",
    "num_features = X.shape[2]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the LSTM model\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, num_features)), \n",
    "    LSTM(300, return_sequences=True),\n",
    "    LSTM(300, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "print(scaled_data)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.5), loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, verbose=1, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "#Save the model and scalers\n",
    "with open('./model/predict_scalers_XAUUSD.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "model.save('./model/predict_model_XAUUSD.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ta\n",
    "symbol = 'BTCUSDT'\n",
    "# df = pd.read_csv('./train-data/OANDA_XAUUSD_Historical.csv')\n",
    "df = pd.read_csv(f'./train-data/OANDA_XAUUSD_Historical.csv')\n",
    "range_value = 15\n",
    "# Xử lý giá trị thiếu\n",
    "df.dropna(inplace=True)\n",
    "lags = range(0, range_value)\n",
    "features = [f'body_{i}' for i in lags]\n",
    "features += [f'shadow_top_{i}' for i in lags]\n",
    "features += [f'shadow_bottom_{i}' for i in lags]\n",
    "# features += [f'volume_avg_{i}' for i in lags]\n",
    "\n",
    "# Chọn đặc trưng và biến mục tiêu\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# Chia dữ liệu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.000001, shuffle=False)\n",
    "\n",
    "# Khởi tạo và huấn luyện mô hình\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Dự đoán và đánh giá\n",
    "# y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# # Dự đoán giá tiếp theo\n",
    "# latest_data = X.iloc[-1].values.reshape(1, -1)\n",
    "# next_movement = rf_classifier.predict(latest_data)\n",
    "model_filename = f'./model_XAUUSD.pkl'\n",
    "\n",
    "\n",
    "joblib.dump(rf_classifier, model_filename)\n",
    "# if next_movement[0] > 0:\n",
    "#     print(\"Giá dự kiến sẽ TĂNG vào ngày tiếp theo.\")\n",
    "# else:\n",
    "#     print(\"Giá dự kiến sẽ GIẢM vào ngày tiếp theo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([-3, 5, -1, 0, 7, -6, 2])\n",
    "\n",
    "count_negative = np.sum(arr < 0)\n",
    "count_positive = np.sum(arr > 0)\n",
    "\n",
    "print(\"Số phần tử < 0:\", count_negative)\n",
    "print(\"Số phần tử > 0:\", count_positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columns = [\"close\", \"high\", \"low\", \"open\"]\n",
    "\n",
    "all_X, all_y = [], []\n",
    "\n",
    "# Create the sliding window dataset with multiple features\n",
    "def create_multifeature_dataset(dataset, time_step=60):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "with open('./scalers/scalers_1.pkl', 'rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "\n",
    "# scalers = {}\n",
    "# scalers['Price'] = MinMaxScaler(feature_range=(0, 1))\n",
    "time_step = 60\n",
    "\n",
    "file_paths = [\n",
    "    './data/OANDA_XAUUSD_Historical.csv',\n",
    "    './data/OANDA_XAUUSD_Historical_2.csv',\n",
    "    './data/OANDA_XAUUSD_Historical_3.csv',\n",
    "]\n",
    "\n",
    "for file in file_paths:\n",
    "    data = pd.read_csv(file)\n",
    "    data = data[columns].dropna()\n",
    "    features = data.values\n",
    "\n",
    "    scaled_data = np.zeros_like(features).astype(float)\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        scaled_data[:, i] = scalers['Price'].transform(features[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "    X, y = create_multifeature_dataset(scaled_data, time_step)\n",
    "\n",
    "    all_X.append(X)\n",
    "    all_y.append(y)\n",
    "\n",
    "X_combined = np.concatenate(all_X, axis=0)\n",
    "Y_combined = np.concatenate(all_y, axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, Y_combined, test_size=0.2, random_state=42)\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, num_features)),\n",
    "    LSTM(200, return_sequences=True),\n",
    "    LayerNormalization(), \n",
    "    LSTM(200, return_sequences=False),\n",
    "    LayerNormalization(), \n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "print(scaled_data)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[early_stopping], batch_size=60, verbose=1, epochs=1000)\n",
    "\n",
    "# Lưu mô hình\n",
    "model.save('./predict_model_fx_3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta.volatility\n",
    "import ta.volume\n",
    "# Đọc dữ liệu từ file CSV\n",
    "data = pd.read_csv('./data/XAUUSD/data43_updated.csv')\n",
    "\n",
    "# Tạo cột 'Next_Close' bằng cách dịch chuyển cột 'Close' lên trên một hàng\n",
    "RSI = ta.momentum.RSIIndicator(data['Close'])\n",
    "MACD = ta.trend.MACD(data['Close'])\n",
    "STOCH = ta.momentum.StochasticOscillator(data['High'], data['Low'], data['Close'])\n",
    "ROC = ta.momentum.ROCIndicator(close=data['Close'], window=12)\n",
    "# SMA = ta.trend.SMAIndicator(close=data['Close'], window=60)\n",
    "ATR = ta.volatility.AverageTrueRange(high=data['High'], low=data['Low'], close=data['Close'], window=14)\n",
    "\n",
    "# data['RSI'] = RSI.rsi()\n",
    "# data['MACD'] = MACD.macd()\n",
    "# data['STOCH_K'] = STOCH.stoch()\n",
    "# data['STOCH_D'] = STOCH.stoch_signal()\n",
    "# data['ROC_RATE'] = ROC.roc()\n",
    "# data['ATR'] = ATR.average_true_range()\n",
    "data['isIncrease'] = data.apply(lambda row: 1 if row['Close'] > row['Open'] else 0, axis=1)\n",
    "increment = 200\n",
    "data['Close'] = data['Close'] + increment\n",
    "data['High'] = data['High'] + increment\n",
    "data['Low'] = data['Low'] + increment\n",
    "data['Open'] = data['Open'] + increment\n",
    "\n",
    "\n",
    "# columns = [\"time\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"RSI\",\"MACD\",\"MACD_SIGNAL\",\"MACD_DIFF\",\"isIncrease\"]\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = './data/XAUUSD/data43_updated.csv'\n",
    "data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta.volatility\n",
    "import ta.volume\n",
    "# Đọc dữ liệu từ file CSV\n",
    "data = pd.read_csv('./data/OANDA_XAUUSD_Historical.csv')\n",
    "\n",
    "increment = 1.4\n",
    "\n",
    "# Chuyển đổi các cột thành số thập phân, nhân với 1.5 và làm tròn đến 1 chữ số thập phân\n",
    "data['close'] = np.round(data['close'].astype(float) * increment, 4)\n",
    "data['low'] = np.round(data['low'].astype(float) * increment, 4)\n",
    "data['high'] = np.round(data['high'].astype(float) * increment, 4)\n",
    "data['open'] = np.round(data['open'].astype(float) * increment, 4)\n",
    "\n",
    "# columns = [\"time\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"RSI\",\"MACD\",\"MACD_SIGNAL\",\"MACD_DIFF\",\"isIncrease\"]\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = './data/OANDA_XAUUSD_Historical_3.csv'\n",
    "data.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau # type: ignore\n",
    "# Load the model and scalers\n",
    "model = load_model('./predict_model_fx_4.h5')\n",
    "\n",
    "with open('./scalers/scalers_1.pkl', 'rb') as f:  \n",
    "    scalers = pickle.load(f) \n",
    "\n",
    "columns = [\"close\", \"high\", \"low\", \"open\"]\n",
    "\n",
    "data = pd.read_csv('./data/OANDA_XAUUSD_Historical_Updated.csv')\n",
    "data = data[columns].dropna()\n",
    "features = data.values\n",
    "scaled_data = np.zeros_like(features).astype(float)\n",
    "\n",
    "# Scale other features separately\n",
    "for i, column in enumerate(columns):\n",
    "    scaled_data[:, i] = scalers['Price'].transform(features[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "def create_multifeature_dataset(dataset, time_step=60):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        Y.append(dataset[i + time_step, 0])  \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 14\n",
    "X, y = create_multifeature_dataset(scaled_data, time_step)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=48, verbose=1, epochs=100000)\n",
    "\n",
    "model.save('./predict_model_fx_5.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import joblib\n",
    "range_value = 10\n",
    "time = 15\n",
    "# Đọc dữ liệu từ các file CSV\n",
    "df_historical = pd.read_csv(f'./train-data/OANDA_XAUUSD_1m_Historical.csv')\n",
    "window_size = 10\n",
    "columns = ['stoch']\n",
    "\n",
    "# Tạo segments cho cả 'body' và 'RSI'\n",
    "def create_segments(data):\n",
    "    return np.array([\n",
    "        np.hstack([data[col].values[i:i + window_size] for col in columns])\n",
    "        for i in range(len(data) - window_size + 1)\n",
    "    ])\n",
    "\n",
    "# Tạo segments cho indicator\n",
    "historical_segments = create_segments(data=df_historical)\n",
    "\n",
    "# Khởi tạo mô hình NearestNeighbors với hàm khoảng cách 'euclidean'\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean')\n",
    "\n",
    "# Huấn luyện mô hình với dữ liệu historical_segmentsz\n",
    "nearest_neighbors.fit(historical_segments)\n",
    "\n",
    "# Lưu mô hình đã huấn luyện ra file để sử dụng lại sau này\n",
    "model_filename = f'nearest_neighbors_model.joblib'\n",
    "joblib.dump(nearest_neighbors, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib\n",
    "\n",
    "time = 5\n",
    "# Đọc dữ liệu từ các file CSV\n",
    "df_historical = pd.read_csv(f'./OANDA_XAUUSD_{time}m_Historical.csv')\n",
    "window_size = 1\n",
    "indicators = [\n",
    "   \n",
    "]\n",
    "range_value = 10\n",
    "\n",
    "# Khởi tạo mô hình NearestNeighbors với hàm khoảng cách tùy chỉnh\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean')\n",
    "\n",
    "# Tạo segments cho cả 'body' và 'RSI'\n",
    "def create_segments(data, indicator):\n",
    "    columns = [f'{indicator}_{i}' for i in range(range_value)]  \n",
    "\n",
    "    return np.array([\n",
    "        np.hstack([data[col].values[i:i + window_size] for col in columns])\n",
    "        for i in range(len(data) - window_size + 1)\n",
    "    ])\n",
    "for indicator in indicators:\n",
    "    print(f\"Training model for indicator: {indicator}\")\n",
    "\n",
    "    # Tạo segments cho indicator\n",
    "    historical_segments = create_segments(data=df_historical, indicator=indicator)\n",
    "\n",
    "    print(historical_segments)\n",
    "\n",
    "    # Khởi tạo mô hình NearestNeighbors với hàm khoảng cách 'euclidean'\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean')\n",
    "\n",
    "    # Huấn luyện mô hình với dữ liệu historical_segments\n",
    "    nearest_neighbors.fit(historical_segments)\n",
    "\n",
    "    # Lưu mô hình đã huấn luyện ra file để sử dụng lại sau này\n",
    "    model_filename = f'nearest_neighbors_model_XAUUSD_{indicator}.joblib'\n",
    "    joblib.dump(nearest_neighbors, model_filename)\n",
    "\n",
    "    print(f\"Model for {indicator} saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_forex_prediction.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Đọc và tiền xử lý dữ liệu\n",
    "\n",
    "# Đọc dữ liệu từ tệp CSV\n",
    "df = pd.read_csv('train-data/OANDA_XAUUSD_15m_Historical.csv')\n",
    "\n",
    "# Sử dụng giá đóng cửa\n",
    "data = df['STOCH'].values\n",
    "\n",
    "# Tiền xử lý dữ liệu\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# 2. Tạo dữ liệu huấn luyện và kiểm tra\n",
    "\n",
    "sequence_length = 60  # Số bước thời gian để dự đoán\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và kiểm tra\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size - sequence_length:]\n",
    "\n",
    "# Hàm tạo dữ liệu chuỗi thời gian\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i - seq_length:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Tạo dữ liệu huấn luyện\n",
    "X_train, y_train = create_sequences(train_data, sequence_length)\n",
    "\n",
    "# Tạo dữ liệu kiểm tra\n",
    "X_test, y_test = create_sequences(test_data, sequence_length)\n",
    "\n",
    "# Thêm chiều cho dữ liệu (nếu cần)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# 3. Xây dựng mô hình Transformer\n",
    "\n",
    "# Hàm tạo lớp Positional Encoding\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model)\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Áp dụng sin cho các vị trí chẵn\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Áp dụng cos cho các vị trí lẻ\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Hàm tạo lớp Transformer Block\n",
    "def transformer_block(inputs, num_heads, ff_dim):\n",
    "    attention_output = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=inputs.shape[-1]\n",
    "    )(inputs, inputs)\n",
    "    attention_output = keras.layers.Dropout(0.1)(attention_output)\n",
    "    out1 = keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    ffn_output = keras.layers.Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = keras.layers.Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = keras.layers.Dropout(0.1)(ffn_output)\n",
    "    return keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Định nghĩa các tham số mô hình\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "ff_dim = 64\n",
    "\n",
    "# Xây dựng mô hình\n",
    "inputs = keras.layers.Input(shape=(sequence_length, 1))\n",
    "x = keras.layers.Dense(d_model)(inputs)\n",
    "\n",
    "# Thêm Positional Encoding\n",
    "pos_encoding = positional_encoding(sequence_length, d_model)\n",
    "x += pos_encoding[:, :sequence_length, :]\n",
    "\n",
    "# Thêm Transformer Block\n",
    "x = transformer_block(x, num_heads, ff_dim)\n",
    "\n",
    "# Flatten và đưa qua Dense Layer\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "outputs = x\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 4. Biên dịch và huấn luyện mô hình\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# 5. Dự đoán và đánh giá mô hình\n",
    "\n",
    "# Dự đoán trên tập kiểm tra\n",
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Save the model and scalers\n",
    "with open('./model/predict_scalers_XAUUSD.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "model.save('./model/predict_model_XAUUSD.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta.trend\n",
    "import ta.volatility\n",
    "import ta.volume\n",
    "time = 240\n",
    "round_num = 5\n",
    "range_value = 10\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data/OANDA_XAUUSD_{time}m_Historical.csv')\n",
    "\n",
    "new_data['body'] =  np.round(new_data['close'] - new_data['open'], round_num)\n",
    "new_data['shadow'] = np.round(new_data['high'] - new_data['low'], round_num)\n",
    "new_data['target'] = np.where(new_data['close'].shift(-1) > new_data['close'], 1, 0)\n",
    "\n",
    "# # Tạo cột 'Next_Close' bằng cách dịch chuyển cột 'Close' lên trên một hàng\n",
    "STOCH = ta.momentum.StochasticOscillator(new_data['high'], new_data['low'], new_data['close'], fillna=True)\n",
    "TSI = ta.momentum.TSIIndicator(close=new_data['close'])\n",
    "RSI = ta.momentum.RSIIndicator(close=new_data['close'])\n",
    "STOCH_RSI = ta.momentum.StochRSIIndicator(close=new_data['close'])\n",
    "PPO = ta.momentum.PercentagePriceOscillator(close=new_data['close'])\n",
    "PVO = ta.momentum.PercentageVolumeOscillator(volume=new_data['Volume'])\n",
    "UO = ta.momentum.UltimateOscillator(high=new_data['high'], low=new_data['low'], close=new_data['close'])\n",
    "WR = ta.momentum.WilliamsRIndicator(high=new_data['high'], low=new_data['low'], close=new_data['close'])\n",
    "ROC = ta.momentum.ROCIndicator(close=new_data['close'])\n",
    "AO = ta.momentum.AwesomeOscillatorIndicator(high=new_data['high'], low=new_data['low'])\n",
    "\n",
    "# Tạo các cột indicator\n",
    "new_data['STOCH_RSI'] = np.round(STOCH_RSI.stochrsi(), round_num)\n",
    "new_data['TSI'] = np.round(TSI.tsi(), round_num)\n",
    "new_data['RSI'] = np.round( RSI.rsi(), round_num)\n",
    "new_data['STOCH'] = np.round(STOCH.stoch(), round_num)\n",
    "\n",
    "\n",
    "lags = range(0, range_value)\n",
    "\n",
    "columns = [new_data['STOCH'].shift(i).rename(f'STOCH_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "columns = [new_data['TSI'].shift(i).rename(f'TSI_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "columns = [new_data['RSI'].shift(i).rename(f'RSI_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "columns = [new_data['STOCH_RSI'].shift(i).rename(f'STOCH_RSI_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./train-data/OANDA_XAUUSD_{time}m_Historical.csv'\n",
    "new_data.dropna().to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "time = 5\n",
    "round_num = 5\n",
    "range_value = 10\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data/OANDA_XAUUSD_{time}m_Historical.csv', sep=',')\n",
    "new_data = dropna(new_data)\n",
    "new_data = add_all_ta_features(df=new_data, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"Volume\", fillna=True)\n",
    "new_data['target'] = np.where(new_data['close'].shift(-1) > new_data['close'], 1, 0)\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./OANDA_XAUUSD_{time}m_Historical.csv'\n",
    "new_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Thiết lập các tham số\n",
    "time = 15\n",
    "round_num = 5\n",
    "indicator = 'DPO'\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data_XAUUSD.csv')\n",
    "\n",
    "# Tạo các cột 'body', 'shadow', 'target'\n",
    "new_data['body'] = np.round(new_data['close'] - new_data['open'], round_num)\n",
    "new_data['shadow'] = np.round(new_data['high'] - new_data['low'], round_num)\n",
    "new_data['target'] = np.where(new_data['close'].shift(-1).rolling(-1) > new_data['close'], 1, 0)\n",
    "\n",
    "# Tạo các cột lag từ 'close_1' đến 'close_49' (tổng cộng 49 cột)\n",
    "lags = range(0, 50)\n",
    "lagged_data = [new_data['close'].shift(i).rename(f'close_{i}') for i in lags]\n",
    "lagged_df = pd.concat(lagged_data, axis=1)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "new_data = new_data.join(lagged_df)\n",
    "\n",
    "# Xác định danh sách các cột 'close' cần chuẩn hóa\n",
    "close_columns = [f'close_{i}' for i in lags] + ['close']  # Đưa 'close' vào cuối để tránh trùng lặp với 'close_0'\n",
    "\n",
    "# Chuẩn hóa Min-Max theo từng hàng bằng vectorization\n",
    "# Tính min và max cho mỗi hàng\n",
    "row_min = new_data[close_columns].min(axis=1)\n",
    "row_max = new_data[close_columns].max(axis=1)\n",
    "range_val = row_max - row_min\n",
    "\n",
    "# Tránh chia cho 0 bằng cách thay thế range_val bằng 1 ở những hàng range_val = 0\n",
    "range_val_replaced = range_val.replace(0, 1)\n",
    "\n",
    "# Thực hiện chuẩn hóa\n",
    "normalized_df = (new_data[close_columns].subtract(row_min, axis=0)).divide(range_val_replaced, axis=0)\n",
    "\n",
    "# Đổi tên các cột đã chuẩn hóa\n",
    "normalized_df.columns = [f'normalized_{col}' for col in close_columns]\n",
    "\n",
    "# Kết hợp DataFrame gốc với DataFrame đã chuẩn hóa\n",
    "df_final = pd.concat([new_data, normalized_df], axis=1)\n",
    "\n",
    "# Xóa các dòng chứa NaN do tạo lag và chuẩn hóa\n",
    "df_final_clean = df_final.dropna()\n",
    "\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./train-data/OANDA_XAUUSD_{time}m_Historical_v2.csv'\n",
    "df_final_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Chuẩn hóa hoàn thành và lưu vào: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import ta.volume\n",
    "\n",
    "def calculate_shadow_top(row):\n",
    "    if row['close'] > row['open']:  # Nến tăng\n",
    "        return row['high'] - row['close']\n",
    "    else:  # Nến giảm\n",
    "        return row['high'] - row['open']\n",
    "\n",
    "def calculate_shadow_bottom(row):\n",
    "    if row['close'] > row['open']:  # Nến tăng\n",
    "        return row['open'] - row['low']\n",
    "    else:  # Nến giảm\n",
    "        return row['close'] - row['low']\n",
    "\n",
    "\n",
    "# Thiết lập các tham số\n",
    "time = 15\n",
    "round_num = 5\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data/OANDA_XAUUSD_1m_Historical.csv')\n",
    "\n",
    "# Tạo các cột 'body', 'shadow', 'target'\n",
    "new_data['body'] = np.round(new_data['close'] - new_data['open'], round_num)\n",
    "\n",
    "# new_data['shadow_top'] = np.where(new_data['body'] > 0, np.round(new_data['high'] - new_data['low'], round_num), -(np.round(new_data['high'] - new_data['low'], round_num)))\n",
    "new_data['shadow_top'] = new_data.apply(calculate_shadow_top, axis=1)\n",
    "new_data['shadow_bottom'] = new_data.apply(calculate_shadow_bottom, axis=1)\n",
    "new_data['target'] = np.where(new_data['body'].shift(-14).rolling(window=15).mean() >= 0, 1, 0)\n",
    "# new_data['volume_avg'] = new_data['Volume'].rolling(window=15).mean()\n",
    "\n",
    "lags = range(0, 15)\n",
    "lagged_data = [new_data['body'].shift(i).rename(f'body_{i}') for i in lags]\n",
    "lagged_data += [new_data['shadow_top'].shift(i).rename(f'shadow_top_{i}') for i in lags]\n",
    "lagged_data += [new_data['shadow_bottom'].shift(i).rename(f'shadow_bottom_{i}') for i in lags]\n",
    "# lagged_data += [new_data['volume_avg'].shift(i).rename(f'volume_avg_{i}') for i in lags]\n",
    "\n",
    "lagged_df = pd.concat(lagged_data, axis=1)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "new_data = new_data.join(lagged_df)\n",
    "\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./train-data/OANDA_XAUUSD_Historical.csv'\n",
    "new_data.dropna().to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Chuẩn hóa hoàn thành và lưu vào: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
