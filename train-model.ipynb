{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc CSV ban đầu và CSV mới\n",
    "#csv_1 = pd.read_csv('./OANDA_BTCUSD, 15 (9).csv')\n",
    "csv_2 = pd.read_csv('./data/OANDA_XAUUSDNEW_Historical.csv')\n",
    "csv_1 = pd.read_csv('./data/OANDA_XAUUSD_1m_Historical.csv')\n",
    "\n",
    "\n",
    "# Chèn nội dung của CSV mới vào CSV ban đầu\n",
    "\n",
    "csv_ban_dau = pd.concat([csv_2, csv_1], ignore_index=True)\n",
    "\n",
    "# Lưu lại vào file mới\n",
    "csv_ban_dau.to_csv('./data/OANDA_XAUUSD_1m_Historical.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc CSV ban đầu và CSV mới\n",
    "csv_1 = pd.read_csv('./OANDA_XAUUSD, 1.csv')\n",
    "csv_2 = pd.read_csv('./OANDA_XAUUSD, 1 (1).csv')\n",
    "csv_3 = pd.read_csv('./OANDA_XAUUSD, 1 (2).csv')\n",
    "csv_4 = pd.read_csv('./OANDA_XAUUSD, 1 (3).csv')\n",
    "csv_5 = pd.read_csv('./OANDA_XAUUSD, 1 (4).csv')\n",
    "csv_6 = pd.read_csv('./OANDA_XAUUSD, 1 (5).csv')\n",
    "csv_7 = pd.read_csv('./OANDA_XAUUSD, 1 (6).csv')\n",
    "csv_8 = pd.read_csv('./OANDA_XAUUSD, 1 (7).csv')\n",
    "csv_9 = pd.read_csv('./OANDA_XAUUSD, 1 (8).csv')\n",
    "csv_10 = pd.read_csv('./OANDA_XAUUSD, 1 (9).csv')\n",
    "csv_11 = pd.read_csv('./OANDA_XAUUSD, 1 (10).csv')\n",
    "csv_12 = pd.read_csv('./OANDA_XAUUSD, 1 (11).csv')\n",
    "csv_13 = pd.read_csv('./OANDA_XAUUSD, 1 (12).csv')\n",
    "csv_14 = pd.read_csv('./OANDA_XAUUSD, 1 (13).csv')\n",
    "csv_15 = pd.read_csv('./OANDA_XAUUSD, 1 (14).csv')\n",
    "csv_16 = pd.read_csv('./OANDA_XAUUSD, 1 (5).csv')\n",
    "csv_17 = pd.read_csv('./OANDA_XAUUSD, 1 (16).csv')\n",
    "csv_18 = pd.read_csv('./OANDA_XAUUSD, 1 (17).csv')\n",
    "csv_19 = pd.read_csv('./OANDA_XAUUSD, 1 (18).csv')\n",
    "\n",
    "# Chèn nội dung của CSV mới vào CSV ban đầu\n",
    "\n",
    "csv_ban_dau = pd.concat([\n",
    "\n",
    "    csv_19,\n",
    "    csv_18,\n",
    "    csv_17,\n",
    "    csv_16,\n",
    "    csv_15, \n",
    "    csv_14, \n",
    "    csv_13,\n",
    "    csv_12, \n",
    "    csv_11, \n",
    "    csv_10, \n",
    "    csv_9,\n",
    "    csv_8,\n",
    "    csv_7,\n",
    "    csv_6,\n",
    "    csv_5,\n",
    "    csv_4,\n",
    "    csv_3,\n",
    "    csv_2,\n",
    "    csv_1,\n",
    "    ], ignore_index=True\n",
    ")\n",
    "\n",
    "# Lưu lại vào file mới\n",
    "csv_ban_dau.to_csv('./data/OANDA_XAUUSDNEW_Historical.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Định nghĩa các cột cần sử dụng\n",
    "columns = [\"close\", \"open\", \"high\", \"low\"]\n",
    "\n",
    "# Định nghĩa các đường dẫn tới file dữ liệu\n",
    "file_paths = [\n",
    "    './train-data/OANDA_XAUUSD_Historical_Updated.csv',\n",
    "]\n",
    "\n",
    "# Khởi tạo scaler cho từng cột\n",
    "scalers = {}\n",
    "scalers['Price'] = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Tạo danh sách để lưu trữ dữ liệu từ tất cả các file\n",
    "all_data = []\n",
    "\n",
    "# Load và thu thập dữ liệu từ tất cả các file\n",
    "for file in file_paths:\n",
    "    data = pd.read_csv(file)\n",
    "    data = data[columns].dropna()\n",
    "    all_data.append(data)\n",
    "\n",
    "# Kết hợp tất cả dữ liệu lại\n",
    "combined_data = pd.concat(all_data, axis=0)\n",
    "\n",
    "df_melted = combined_data.melt(var_name='Type', value_name='Value')\n",
    "\n",
    "scalers['Price'].fit(df_melted['Value'].values.reshape(-1, 1))\n",
    "    \n",
    "\n",
    "# Lưu các scaler vào file\n",
    "with open('./scalers/scalers_1.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "print(\"Scalers have been fitted and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "\n",
    "\n",
    "columns = [\"body\", \"STOCH_D\", \"STOCH_K\"]\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('./train-data/OANDA_XAUUSD_Historical.csv')\n",
    "features = data[columns].values\n",
    "scaled_data = np.zeros_like(features).astype(float)\n",
    "# Initialize and fit scalers\n",
    "scalers = {}\n",
    "# Scale the features\n",
    "for i, column in enumerate(columns):\n",
    "    scalers[column] = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data[:, i] = scalers[column].fit_transform(features[:, i].reshape(-1, 1)).flatten()\n",
    "# Create the sliding window dataset\n",
    "def create_multifeature_dataset(dataset, time_step=60):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        Y.append(dataset[i + time_step, 0])  \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 20\n",
    "X, y = create_multifeature_dataset(scaled_data, time_step)\n",
    "\n",
    "# Adjust input shape to include features\n",
    "num_features = X.shape[2]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the LSTM model\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, num_features)), \n",
    "    LSTM(300, return_sequences=True),\n",
    "    LSTM(300, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "print(scaled_data)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.5), loss='mean_squared_error')\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, verbose=1, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "#Save the model and scalers\n",
    "with open('./model/predict_scalers_XAUUSD.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "model.save('./model/predict_model_XAUUSD.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# in utils.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# (OpenMP is loaded by importing show_versions right after this block)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadpoolController\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\__init__.py:99\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# This is the first import of an extension module within SciPy. If there's\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# a general issue with the install, such that extension modules are missing\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# or cannot be imported, this is where we'll get a failure - so give an\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# informative error message.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ccallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LowLevelCallable\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `scipy` install you are using seems to be broken, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    102\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(extension modules cannot be imported), \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m    103\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease try reinstalling.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\_lib\\_ccallback.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ccallback_c\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[0;32m      5\u001b[0m PyCFuncPtr \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCFUNCTYPE(ctypes\u001b[38;5;241m.\u001b[39mc_void_p)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__bases__\u001b[39m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ta\n",
    "symbol = 'BTCUSDT'\n",
    "# df = pd.read_csv('./train-data/OANDA_XAUUSD_Historical.csv')\n",
    "df = pd.read_csv(f'./train-data/OANDA_XAUUSD_Historical.csv')\n",
    "range_value = 15\n",
    "# Xử lý giá trị thiếu\n",
    "df.dropna(inplace=True)\n",
    "lags = range(0, range_value)\n",
    "features = [f'body_{i}' for i in lags]\n",
    "features += [f'shadow_top_{i}' for i in lags]\n",
    "features += [f'shadow_bottom_{i}' for i in lags]\n",
    "# features += [f'volume_avg_{i}' for i in lags]\n",
    "\n",
    "# Chọn đặc trưng và biến mục tiêu\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# Chia dữ liệu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "# Khởi tạo và huấn luyện mô hình\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán và đánh giá\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Dự đoán giá tiếp theo\n",
    "latest_data = X.iloc[-1].values.reshape(1, -1)\n",
    "next_movement = rf_classifier.predict(latest_data)\n",
    "model_filename = f'./random_forest_xau_model.pkl'\n",
    "\n",
    "\n",
    "joblib.dump(rf_classifier, model_filename)\n",
    "if next_movement[0] > 0:\n",
    "    print(\"Giá dự kiến sẽ TĂNG vào ngày tiếp theo.\")\n",
    "else:\n",
    "    print(\"Giá dự kiến sẽ GIẢM vào ngày tiếp theo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columns = [\"close\", \"high\", \"low\", \"open\"]\n",
    "\n",
    "all_X, all_y = [], []\n",
    "\n",
    "# Create the sliding window dataset with multiple features\n",
    "def create_multifeature_dataset(dataset, time_step=60):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "with open('./scalers/scalers_1.pkl', 'rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "\n",
    "# scalers = {}\n",
    "# scalers['Price'] = MinMaxScaler(feature_range=(0, 1))\n",
    "time_step = 60\n",
    "\n",
    "file_paths = [\n",
    "    './data/OANDA_XAUUSD_Historical.csv',\n",
    "    './data/OANDA_XAUUSD_Historical_2.csv',\n",
    "    './data/OANDA_XAUUSD_Historical_3.csv',\n",
    "]\n",
    "\n",
    "for file in file_paths:\n",
    "    data = pd.read_csv(file)\n",
    "    data = data[columns].dropna()\n",
    "    features = data.values\n",
    "\n",
    "    scaled_data = np.zeros_like(features).astype(float)\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        scaled_data[:, i] = scalers['Price'].transform(features[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "    X, y = create_multifeature_dataset(scaled_data, time_step)\n",
    "\n",
    "    all_X.append(X)\n",
    "    all_y.append(y)\n",
    "\n",
    "X_combined = np.concatenate(all_X, axis=0)\n",
    "Y_combined = np.concatenate(all_y, axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, Y_combined, test_size=0.2, random_state=42)\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, num_features)),\n",
    "    LSTM(200, return_sequences=True),\n",
    "    LayerNormalization(), \n",
    "    LSTM(200, return_sequences=False),\n",
    "    LayerNormalization(), \n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "print(scaled_data)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),callbacks=[early_stopping], batch_size=60, verbose=1, epochs=1000)\n",
    "\n",
    "# Lưu mô hình\n",
    "model.save('./predict_model_fx_3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta.volatility\n",
    "import ta.volume\n",
    "# Đọc dữ liệu từ file CSV\n",
    "data = pd.read_csv('./data/XAUUSD/data43_updated.csv')\n",
    "\n",
    "# Tạo cột 'Next_Close' bằng cách dịch chuyển cột 'Close' lên trên một hàng\n",
    "RSI = ta.momentum.RSIIndicator(data['Close'])\n",
    "MACD = ta.trend.MACD(data['Close'])\n",
    "STOCH = ta.momentum.StochasticOscillator(data['High'], data['Low'], data['Close'])\n",
    "ROC = ta.momentum.ROCIndicator(close=data['Close'], window=12)\n",
    "# SMA = ta.trend.SMAIndicator(close=data['Close'], window=60)\n",
    "ATR = ta.volatility.AverageTrueRange(high=data['High'], low=data['Low'], close=data['Close'], window=14)\n",
    "\n",
    "# data['RSI'] = RSI.rsi()\n",
    "# data['MACD'] = MACD.macd()\n",
    "# data['STOCH_K'] = STOCH.stoch()\n",
    "# data['STOCH_D'] = STOCH.stoch_signal()\n",
    "# data['ROC_RATE'] = ROC.roc()\n",
    "# data['ATR'] = ATR.average_true_range()\n",
    "data['isIncrease'] = data.apply(lambda row: 1 if row['Close'] > row['Open'] else 0, axis=1)\n",
    "increment = 200\n",
    "data['Close'] = data['Close'] + increment\n",
    "data['High'] = data['High'] + increment\n",
    "data['Low'] = data['Low'] + increment\n",
    "data['Open'] = data['Open'] + increment\n",
    "\n",
    "\n",
    "# columns = [\"time\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"RSI\",\"MACD\",\"MACD_SIGNAL\",\"MACD_DIFF\",\"isIncrease\"]\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = './data/XAUUSD/data43_updated.csv'\n",
    "data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta.volatility\n",
    "import ta.volume\n",
    "# Đọc dữ liệu từ file CSV\n",
    "data = pd.read_csv('./data/OANDA_XAUUSD_Historical.csv')\n",
    "\n",
    "increment = 1.4\n",
    "\n",
    "# Chuyển đổi các cột thành số thập phân, nhân với 1.5 và làm tròn đến 1 chữ số thập phân\n",
    "data['close'] = np.round(data['close'].astype(float) * increment, 4)\n",
    "data['low'] = np.round(data['low'].astype(float) * increment, 4)\n",
    "data['high'] = np.round(data['high'].astype(float) * increment, 4)\n",
    "data['open'] = np.round(data['open'].astype(float) * increment, 4)\n",
    "\n",
    "# columns = [\"time\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"RSI\",\"MACD\",\"MACD_SIGNAL\",\"MACD_DIFF\",\"isIncrease\"]\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = './data/OANDA_XAUUSD_Historical_3.csv'\n",
    "data.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau # type: ignore\n",
    "# Load the model and scalers\n",
    "model = load_model('./predict_model_fx_4.h5')\n",
    "\n",
    "with open('./scalers/scalers_1.pkl', 'rb') as f:  \n",
    "    scalers = pickle.load(f) \n",
    "\n",
    "columns = [\"close\", \"high\", \"low\", \"open\"]\n",
    "\n",
    "data = pd.read_csv('./data/OANDA_XAUUSD_Historical_Updated.csv')\n",
    "data = data[columns].dropna()\n",
    "features = data.values\n",
    "scaled_data = np.zeros_like(features).astype(float)\n",
    "\n",
    "# Scale other features separately\n",
    "for i, column in enumerate(columns):\n",
    "    scaled_data[:, i] = scalers['Price'].transform(features[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "def create_multifeature_dataset(dataset, time_step=60):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        X.append(dataset[i:(i + time_step), :])\n",
    "        Y.append(dataset[i + time_step, 0])  \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 14\n",
    "X, y = create_multifeature_dataset(scaled_data, time_step)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=48, verbose=1, epochs=100000)\n",
    "\n",
    "model.save('./predict_model_fx_5.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      6\u001b[0m range_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\__init__.py:157\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Set a global controller that can be used to locally limit the number of\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# threads without looping through all shared libraries every time.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# This instantitation should not happen earlier because it needs all BLAS and\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# OpenMP libs to be loaded first.\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m     _threadpool_controller \u001b[38;5;241m=\u001b[39m \u001b[43mThreadpoolController\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_module\u001b[39m(module):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fixture for the tests to assure globally controllable seeding of RNGs\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\threadpoolctl.py:818\u001b[0m, in \u001b[0;36mThreadpoolController.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib_controllers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\threadpoolctl.py:972\u001b[0m, in \u001b[0;36mThreadpoolController._load_libraries\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_libraries_with_dyld()\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 972\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_libraries_with_enum_process_module_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyodide\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_libraries_pyodide()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\threadpoolctl.py:1100\u001b[0m, in \u001b[0;36mThreadpoolController._find_libraries_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         filepath \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;66;03m# Store the library controller if it is supported and selected\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_controller_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     kernel_32\u001b[38;5;241m.\u001b[39mCloseHandle(h_process)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\threadpoolctl.py:1134\u001b[0m, in \u001b[0;36mThreadpoolController._make_controller_from_path\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Store a library controller if it is supported and selected\"\"\"\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# Required to resolve symlinks\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[43m_realpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# `lower` required to take account of OpenMP dll case on Windows\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# (vcomp, VCOMP, Vcomp, ...)\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(filepath)\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\threadpoolctl.py:548\u001b[0m, in \u001b[0;36m_realpath\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_realpath\u001b[39m(filepath):\n\u001b[0;32m    547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Small caching wrapper around os.path.realpath to limit system calls\"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen ntpath>:749\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import joblib\n",
    "range_value = 10\n",
    "time = 15\n",
    "# Đọc dữ liệu từ các file CSV\n",
    "df_historical = pd.read_csv(f'./train-data/OANDA_XAUUSD_1m_Historical.csv')\n",
    "window_size = 10\n",
    "columns = ['stoch']\n",
    "\n",
    "# Tạo segments cho cả 'body' và 'RSI'\n",
    "def create_segments(data):\n",
    "    return np.array([\n",
    "        np.hstack([data[col].values[i:i + window_size] for col in columns])\n",
    "        for i in range(len(data) - window_size + 1)\n",
    "    ])\n",
    "\n",
    "# Tạo segments cho indicator\n",
    "historical_segments = create_segments(data=df_historical)\n",
    "\n",
    "# Khởi tạo mô hình NearestNeighbors với hàm khoảng cách 'euclidean'\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean')\n",
    "\n",
    "# Huấn luyện mô hình với dữ liệu historical_segmentsz\n",
    "nearest_neighbors.fit(historical_segments)\n",
    "\n",
    "# Lưu mô hình đã huấn luyện ra file để sử dụng lại sau này\n",
    "model_filename = f'nearest_neighbors_model.joblib'\n",
    "joblib.dump(nearest_neighbors, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib\n",
    "\n",
    "time = 5\n",
    "# Đọc dữ liệu từ các file CSV\n",
    "df_historical = pd.read_csv(f'./OANDA_XAUUSD_{time}m_Historical.csv')\n",
    "window_size = 1\n",
    "indicators = [\n",
    "   \n",
    "]\n",
    "range_value = 10\n",
    "\n",
    "# Khởi tạo mô hình NearestNeighbors với hàm khoảng cách tùy chỉnh\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean')\n",
    "\n",
    "# Tạo segments cho cả 'body' và 'RSI'\n",
    "def create_segments(data, indicator):\n",
    "    columns = [f'{indicator}_{i}' for i in range(range_value)]  \n",
    "\n",
    "    return np.array([\n",
    "        np.hstack([data[col].values[i:i + window_size] for col in columns])\n",
    "        for i in range(len(data) - window_size + 1)\n",
    "    ])\n",
    "for indicator in indicators:\n",
    "    print(f\"Training model for indicator: {indicator}\")\n",
    "\n",
    "    # Tạo segments cho indicator\n",
    "    historical_segments = create_segments(data=df_historical, indicator=indicator)\n",
    "\n",
    "    print(historical_segments)\n",
    "\n",
    "    # Khởi tạo mô hình NearestNeighbors với hàm khoảng cách 'euclidean'\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean')\n",
    "\n",
    "    # Huấn luyện mô hình với dữ liệu historical_segments\n",
    "    nearest_neighbors.fit(historical_segments)\n",
    "\n",
    "    # Lưu mô hình đã huấn luyện ra file để sử dụng lại sau này\n",
    "    model_filename = f'nearest_neighbors_model_XAUUSD_{indicator}.joblib'\n",
    "    joblib.dump(nearest_neighbors, model_filename)\n",
    "\n",
    "    print(f\"Model for {indicator} saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_forex_prediction.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Đọc và tiền xử lý dữ liệu\n",
    "\n",
    "# Đọc dữ liệu từ tệp CSV\n",
    "df = pd.read_csv('train-data/OANDA_XAUUSD_15m_Historical.csv')\n",
    "\n",
    "# Sử dụng giá đóng cửa\n",
    "data = df['STOCH'].values\n",
    "\n",
    "# Tiền xử lý dữ liệu\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# 2. Tạo dữ liệu huấn luyện và kiểm tra\n",
    "\n",
    "sequence_length = 60  # Số bước thời gian để dự đoán\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và kiểm tra\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size - sequence_length:]\n",
    "\n",
    "# Hàm tạo dữ liệu chuỗi thời gian\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i - seq_length:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Tạo dữ liệu huấn luyện\n",
    "X_train, y_train = create_sequences(train_data, sequence_length)\n",
    "\n",
    "# Tạo dữ liệu kiểm tra\n",
    "X_test, y_test = create_sequences(test_data, sequence_length)\n",
    "\n",
    "# Thêm chiều cho dữ liệu (nếu cần)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# 3. Xây dựng mô hình Transformer\n",
    "\n",
    "# Hàm tạo lớp Positional Encoding\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model)\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Áp dụng sin cho các vị trí chẵn\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Áp dụng cos cho các vị trí lẻ\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Hàm tạo lớp Transformer Block\n",
    "def transformer_block(inputs, num_heads, ff_dim):\n",
    "    attention_output = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=inputs.shape[-1]\n",
    "    )(inputs, inputs)\n",
    "    attention_output = keras.layers.Dropout(0.1)(attention_output)\n",
    "    out1 = keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    ffn_output = keras.layers.Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = keras.layers.Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = keras.layers.Dropout(0.1)(ffn_output)\n",
    "    return keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Định nghĩa các tham số mô hình\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "ff_dim = 64\n",
    "\n",
    "# Xây dựng mô hình\n",
    "inputs = keras.layers.Input(shape=(sequence_length, 1))\n",
    "x = keras.layers.Dense(d_model)(inputs)\n",
    "\n",
    "# Thêm Positional Encoding\n",
    "pos_encoding = positional_encoding(sequence_length, d_model)\n",
    "x += pos_encoding[:, :sequence_length, :]\n",
    "\n",
    "# Thêm Transformer Block\n",
    "x = transformer_block(x, num_heads, ff_dim)\n",
    "\n",
    "# Flatten và đưa qua Dense Layer\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "outputs = x\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 4. Biên dịch và huấn luyện mô hình\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# 5. Dự đoán và đánh giá mô hình\n",
    "\n",
    "# Dự đoán trên tập kiểm tra\n",
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Save the model and scalers\n",
    "with open('./model/predict_scalers_XAUUSD.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "model.save('./model/predict_model_XAUUSD.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ta.trend\n",
    "import ta.volatility\n",
    "import ta.volume\n",
    "time = 240\n",
    "round_num = 5\n",
    "range_value = 10\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data/OANDA_XAUUSD_{time}m_Historical.csv')\n",
    "\n",
    "new_data['body'] =  np.round(new_data['close'] - new_data['open'], round_num)\n",
    "new_data['shadow'] = np.round(new_data['high'] - new_data['low'], round_num)\n",
    "new_data['target'] = np.where(new_data['close'].shift(-1) > new_data['close'], 1, 0)\n",
    "\n",
    "# # Tạo cột 'Next_Close' bằng cách dịch chuyển cột 'Close' lên trên một hàng\n",
    "STOCH = ta.momentum.StochasticOscillator(new_data['high'], new_data['low'], new_data['close'], fillna=True)\n",
    "TSI = ta.momentum.TSIIndicator(close=new_data['close'])\n",
    "RSI = ta.momentum.RSIIndicator(close=new_data['close'])\n",
    "STOCH_RSI = ta.momentum.StochRSIIndicator(close=new_data['close'])\n",
    "PPO = ta.momentum.PercentagePriceOscillator(close=new_data['close'])\n",
    "PVO = ta.momentum.PercentageVolumeOscillator(volume=new_data['Volume'])\n",
    "UO = ta.momentum.UltimateOscillator(high=new_data['high'], low=new_data['low'], close=new_data['close'])\n",
    "WR = ta.momentum.WilliamsRIndicator(high=new_data['high'], low=new_data['low'], close=new_data['close'])\n",
    "ROC = ta.momentum.ROCIndicator(close=new_data['close'])\n",
    "AO = ta.momentum.AwesomeOscillatorIndicator(high=new_data['high'], low=new_data['low'])\n",
    "\n",
    "# Tạo các cột indicator\n",
    "new_data['STOCH_RSI'] = np.round(STOCH_RSI.stochrsi(), round_num)\n",
    "new_data['TSI'] = np.round(TSI.tsi(), round_num)\n",
    "new_data['RSI'] = np.round( RSI.rsi(), round_num)\n",
    "new_data['STOCH'] = np.round(STOCH.stoch(), round_num)\n",
    "\n",
    "\n",
    "lags = range(0, range_value)\n",
    "\n",
    "columns = [new_data['STOCH'].shift(i).rename(f'STOCH_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "columns = [new_data['TSI'].shift(i).rename(f'TSI_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "columns = [new_data['RSI'].shift(i).rename(f'RSI_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "columns = [new_data['STOCH_RSI'].shift(i).rename(f'STOCH_RSI_{i}') for i in lags]\n",
    "df = pd.concat(columns, axis=1)\n",
    "new_data = new_data.join(df)\n",
    "\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./train-data/OANDA_XAUUSD_{time}m_Historical.csv'\n",
    "new_data.dropna().to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "time = 5\n",
    "round_num = 5\n",
    "range_value = 10\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data/OANDA_XAUUSD_{time}m_Historical.csv', sep=',')\n",
    "new_data = dropna(new_data)\n",
    "new_data = add_all_ta_features(df=new_data, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"Volume\", fillna=True)\n",
    "new_data['target'] = np.where(new_data['close'].shift(-1) > new_data['close'], 1, 0)\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./OANDA_XAUUSD_{time}m_Historical.csv'\n",
    "new_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Thiết lập các tham số\n",
    "time = 15\n",
    "round_num = 5\n",
    "indicator = 'DPO'\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data_XAUUSD.csv')\n",
    "\n",
    "# Tạo các cột 'body', 'shadow', 'target'\n",
    "new_data['body'] = np.round(new_data['close'] - new_data['open'], round_num)\n",
    "new_data['shadow'] = np.round(new_data['high'] - new_data['low'], round_num)\n",
    "new_data['target'] = np.where(new_data['close'].shift(-1).rolling(-1) > new_data['close'], 1, 0)\n",
    "\n",
    "# Tạo các cột lag từ 'close_1' đến 'close_49' (tổng cộng 49 cột)\n",
    "lags = range(0, 50)\n",
    "lagged_data = [new_data['close'].shift(i).rename(f'close_{i}') for i in lags]\n",
    "lagged_df = pd.concat(lagged_data, axis=1)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "new_data = new_data.join(lagged_df)\n",
    "\n",
    "# Xác định danh sách các cột 'close' cần chuẩn hóa\n",
    "close_columns = [f'close_{i}' for i in lags] + ['close']  # Đưa 'close' vào cuối để tránh trùng lặp với 'close_0'\n",
    "\n",
    "# Chuẩn hóa Min-Max theo từng hàng bằng vectorization\n",
    "# Tính min và max cho mỗi hàng\n",
    "row_min = new_data[close_columns].min(axis=1)\n",
    "row_max = new_data[close_columns].max(axis=1)\n",
    "range_val = row_max - row_min\n",
    "\n",
    "# Tránh chia cho 0 bằng cách thay thế range_val bằng 1 ở những hàng range_val = 0\n",
    "range_val_replaced = range_val.replace(0, 1)\n",
    "\n",
    "# Thực hiện chuẩn hóa\n",
    "normalized_df = (new_data[close_columns].subtract(row_min, axis=0)).divide(range_val_replaced, axis=0)\n",
    "\n",
    "# Đổi tên các cột đã chuẩn hóa\n",
    "normalized_df.columns = [f'normalized_{col}' for col in close_columns]\n",
    "\n",
    "# Kết hợp DataFrame gốc với DataFrame đã chuẩn hóa\n",
    "df_final = pd.concat([new_data, normalized_df], axis=1)\n",
    "\n",
    "# Xóa các dòng chứa NaN do tạo lag và chuẩn hóa\n",
    "df_final_clean = df_final.dropna()\n",
    "\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./train-data/OANDA_XAUUSD_{time}m_Historical_v2.csv'\n",
    "df_final_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Chuẩn hóa hoàn thành và lưu vào: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chuẩn hóa hoàn thành và lưu vào: ./train-data/OANDA_XAUUSD_Historical.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "import ta.momentum\n",
    "import ta.trend\n",
    "import ta.volume\n",
    "\n",
    "def calculate_shadow_top(row):\n",
    "    if row['close'] > row['open']:  # Nến tăng\n",
    "        return row['high'] - row['close']\n",
    "    else:  # Nến giảm\n",
    "        return row['high'] - row['open']\n",
    "\n",
    "def calculate_shadow_bottom(row):\n",
    "    if row['close'] > row['open']:  # Nến tăng\n",
    "        return row['open'] - row['low']\n",
    "    else:  # Nến giảm\n",
    "        return row['close'] - row['low']\n",
    "\n",
    "\n",
    "# Thiết lập các tham số\n",
    "time = 15\n",
    "round_num = 5\n",
    "# Đọc dữ liệu từ file CSV\n",
    "new_data = pd.read_csv(f'./data/OANDA_XAUUSD_1m_Historical.csv')\n",
    "\n",
    "# Tạo các cột 'body', 'shadow', 'target'\n",
    "new_data['body'] = np.round(new_data['close'] - new_data['open'], round_num)\n",
    "\n",
    "# new_data['shadow_top'] = np.where(new_data['body'] > 0, np.round(new_data['high'] - new_data['low'], round_num), -(np.round(new_data['high'] - new_data['low'], round_num)))\n",
    "new_data['shadow_top'] = new_data.apply(calculate_shadow_top, axis=1)\n",
    "new_data['shadow_bottom'] = new_data.apply(calculate_shadow_bottom, axis=1)\n",
    "new_data['target'] = np.where(new_data['body'].shift(-14).rolling(window=15).mean() >= 0, 1, 0)\n",
    "# new_data['volume_avg'] = new_data['Volume'].rolling(window=15).mean()\n",
    "\n",
    "lags = range(0, 15)\n",
    "lagged_data = [new_data['body'].shift(i).rename(f'body_{i}') for i in lags]\n",
    "lagged_data += [new_data['shadow_top'].shift(i).rename(f'shadow_top_{i}') for i in lags]\n",
    "lagged_data += [new_data['shadow_bottom'].shift(i).rename(f'shadow_bottom_{i}') for i in lags]\n",
    "# lagged_data += [new_data['volume_avg'].shift(i).rename(f'volume_avg_{i}') for i in lags]\n",
    "\n",
    "lagged_df = pd.concat(lagged_data, axis=1)\n",
    "\n",
    "# Kết hợp DataFrame mới với DataFrame gốc\n",
    "new_data = new_data.join(lagged_df)\n",
    "\n",
    "# Lưu dữ liệu đã xử lý vào file CSV mới\n",
    "output_path = f'./train-data/OANDA_XAUUSD_Historical.csv'\n",
    "new_data.dropna().to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Chuẩn hóa hoàn thành và lưu vào: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
