{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Load dữ liệu\n",
    "data = pd.read_csv('./train-data/FINANCIAL_DATA.csv')\n",
    "# Hàm tạo dataset dạng sliding window với target đã có sẵn (không dùng scaler)\n",
    "def create_multifeature_dataset(df):\n",
    "    X = df[columns].values\n",
    "    Y = df['target'].values\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "time_step = 1\n",
    "X, y = create_multifeature_dataset(data)\n",
    "\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Chia dữ liệu thành tập train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, len(columns))),\n",
    "    LSTM(300, return_sequences=True),\n",
    "    LSTM(300, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Dùng hàm kích hoạt sigmoid cho đầu ra nhị phân\n",
    "])\n",
    "\n",
    "# Compile model với loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Định nghĩa callback dừng sớm nếu val_accuracy không cải thiện\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "          batch_size=64, verbose=1, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "# Lưu model (không lưu scaler vì không sử dụng)\n",
    "model.save('./model/predict_model_XAUUSD_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    # Lấy các cột features theo thứ tự mong muốn\n",
    "    features = [batch[col] for col in columns]  # mỗi tensor có shape (batch_size,)\n",
    "    X = tf.stack(features, axis=1)              # shape: (batch_size, num_features)\n",
    "    X = tf.expand_dims(X, axis=1)                 # reshape thành (batch_size, time_steps=1, num_features)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "dataset = raw_dataset.map(parse_csv)\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset   = val_dataset.batch(batch_size)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, len(columns))),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    steps = []\n",
    "    # Lặp qua các lag từ 0 đến 14 để tạo ra 15 time step\n",
    "    for lag in range(0, 15):\n",
    "        # Mỗi time step có 3 đặc trưng: body, shadow_top, shadow_bottom\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # Kết quả: shape (batch_size, 3)\n",
    "        steps.append(step)\n",
    "    # Stack các time step lại theo chiều time: kết quả shape (batch_size, 15, 3)\n",
    "    X = tf.stack(steps, axis=1)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "dataset = raw_dataset.map(parse_csv).cache('/tmp/FOREX_cache').prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset   = val_dataset.batch(batch_size)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LST\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(15, 3)),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(15)\n",
    "columns = [f'{col}_{lag}' for col in ['body', 'shadow_top', 'shadow_bottom'] for lag in lags]\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    steps = []\n",
    "    for lag in range(15):\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # (batch_size, 3)\n",
    "        steps.append(step)\n",
    "    X = tf.stack(steps, axis=1)  # (batch_size, 15, 3)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Sử dụng parallel map\n",
    "dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Cache kết quả sau map để tránh xử lý lại trong các epoch sau\n",
    "dataset = dataset.cache('/tmp/FOREX_cache')\n",
    "# Prefetch để giảm latency\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. Chuyển đổi thành từng mẫu đơn\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# 5. Chia dữ liệu sử dụng shard (thay cho enumerate+filter)\n",
    "# Giả sử chia thành 5 shard: 80% cho train (shard 0,1,2,3) và 20% cho validation (shard 4)\n",
    "# Phương án này đòi hỏi dữ liệu đã được xáo trộn\n",
    "train_shards = [dataset.shard(num_shards=5, index=i) for i in range(4)]\n",
    "train_dataset = train_shards[0]\n",
    "for shard in train_shards[1:]:\n",
    "    train_dataset = train_dataset.concatenate(shard)\n",
    "\n",
    "val_dataset = dataset.shard(num_shards=5, index=4)\n",
    "\n",
    "# 6. Re-batch, repeat và prefetch\n",
    "train_dataset = train_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 7. Tính số bước (nếu cần tính toán bước dựa trên số dòng)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(15, 3)),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Lưu model\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m143647/268638\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6:07\u001b[0m 3ms/step - accuracy: 0.5784 - loss: 0.6754 - precision: 0.5795"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    # Lấy các cột features theo thứ tự mong muốn\n",
    "    features = [batch[col] for col in columns]  # mỗi tensor có shape (batch_size,)\n",
    "    X = tf.stack(features, axis=1)              # shape: (batch_size, num_features)\n",
    "    X = tf.expand_dims(X, axis=1)                 # reshape thành (batch_size, time_steps=1, num_features)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "dataset = raw_dataset.map(parse_csv)\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "# 8. Xây dựng mô hình MLP thay vì LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, len(columns))),\n",
    "    tf.keras.layers.Flatten(),  # chuyển đổi từ (batch_size, 1, num_features) -> (batch_size, num_features)\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Đọc file CSV lớn; Dask tự động chia thành nhiều partition\n",
    "df = dd.read_csv('./train-data/FOREX.csv')\n",
    "\n",
    "# Xáo trộn toàn bộ dữ liệu (lấy mẫu ngẫu nhiên với frac=1)\n",
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Gom tất cả các partition thành 1 partition\n",
    "df_single = df_shuffled.repartition(npartitions=1)\n",
    "\n",
    "# Ghi ra file CSV duy nhất (tham số single_file=True yêu cầu Dask phiên bản hỗ trợ)\n",
    "df_single.to_csv('FOREX.csv', index=False, single_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Cài đặt tf.distribute để chạy song song trên GPU\n",
    "strategy = tf.distribute.MirroredStrategy()  # Sử dụng tất cả các GPU có sẵn\n",
    "\n",
    "# Đường dẫn đến file CSV và thiết lập batch size\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 2. Xây dựng danh sách các cột features với các lag\n",
    "lags = list(range(15))\n",
    "def get_columns_for_lag(lag):\n",
    "    return [f'body_{lag}', f'shadow_top_{lag}', f'shadow_bottom_{lag}']\n",
    "\n",
    "columns = []\n",
    "for lag in lags:\n",
    "    columns.extend(get_columns_for_lag(lag))\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 3. Tạo dataset streaming từ CSV (sử dụng CPU)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "def parse_csv(batch):\n",
    "    # Xây dựng dữ liệu với shape: (batch_size, time_steps=15, features=3)\n",
    "    X_steps = []\n",
    "    for lag in lags:\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # shape: (batch_size, 3)\n",
    "        X_steps.append(step)\n",
    "    X = tf.stack(X_steps, axis=1)  # shape: (batch_size, 15, 3)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Áp dụng map để chuyển đổi dữ liệu\n",
    "dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Tính số lượng batch theo số dòng (nếu cần)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "total_steps = total_samples // batch_size\n",
    "train_batches = int(0.8 * total_steps)\n",
    "val_batches = total_steps - train_batches\n",
    "\n",
    "# Phân chia tập train và validation bằng take và skip\n",
    "train_dataset = dataset.take(train_batches).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = dataset.skip(train_batches).take(val_batches).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. Xây dựng mô hình Transformer trong phạm vi của strategy\n",
    "with strategy.scope():\n",
    "    # Tham số mô hình\n",
    "    time_steps = len(lags)       # 15\n",
    "    num_features = 3             # mỗi time step có 3 features\n",
    "    embedding_dim = 64           # chiều embedding cho mỗi time step\n",
    "    num_transformer_blocks = 2   # số Transformer Encoder block\n",
    "    head_size = 64\n",
    "    num_heads = 4\n",
    "    ff_dim = 128\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    # Lớp Positional Embedding\n",
    "    class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "        def __init__(self, sequence_length, output_dim):\n",
    "            super(PositionalEmbedding, self).__init__()\n",
    "            self.token_proj = tf.keras.layers.Dense(output_dim)\n",
    "            self.position_embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=output_dim\n",
    "            )\n",
    "            self.sequence_length = sequence_length\n",
    "\n",
    "        def call(self, inputs):\n",
    "            positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "            pos_embed = self.position_embedding(positions)  # (sequence_length, output_dim)\n",
    "            token_embed = self.token_proj(inputs)           # (batch_size, sequence_length, output_dim)\n",
    "            return token_embed + pos_embed\n",
    "\n",
    "    # Định nghĩa Transformer Encoder block\n",
    "    def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout):\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        attn_output = tf.keras.layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )(x, x)\n",
    "        x = tf.keras.layers.Add()([attn_output, inputs])\n",
    "        x_ff = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x_ff = tf.keras.layers.Dense(ff_dim, activation=\"relu\")(x_ff)\n",
    "        x_ff = tf.keras.layers.Dropout(dropout)(x_ff)\n",
    "        x_ff = tf.keras.layers.Dense(inputs.shape[-1])(x_ff)\n",
    "        return tf.keras.layers.Add()([x, x_ff])\n",
    "\n",
    "    # Xây dựng mô hình\n",
    "    inputs = tf.keras.layers.Input(shape=(time_steps, num_features))\n",
    "    x = PositionalEmbedding(sequence_length=time_steps, output_dim=embedding_dim)(inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_precision',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 5. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=1,\n",
    "          steps_per_epoch=train_batches,\n",
    "          validation_steps=val_batches,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 6. Lưu mô hình sau training\n",
    "model.save('./predict_model_XAUUSD_transformer_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = [f'{col}_{lag}' for col in ['body', 'shadow_top', 'shadow_bottom'] for lag in lags]\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV với đọc song song\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    num_parallel_reads=tf.data.experimental.AUTOTUNE\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    steps = []\n",
    "    for lag in range(0, 15):\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # Kết quả shape: (batch_size, 3)\n",
    "        steps.append(step)\n",
    "    # Stack theo chiều thời gian: (batch_size, 15, 3)\n",
    "    X = tf.stack(steps, axis=1)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Áp dụng map với xử lý song song\n",
    "dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 4. Tùy chọn: Nếu dữ liệu đủ nhỏ để cache, có thể cache sau map\n",
    "# dataset = dataset.cache()\n",
    "\n",
    "# 5. Tách dữ liệu thành các mẫu riêng lẻ và sử dụng shard để chia train/validation\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# Sử dụng shard: Chia dữ liệu thành 5 phần, 4 phần train, 1 phần validation\n",
    "train_dataset = dataset.shard(num_shards=5, index=0)\n",
    "for i in range(1, 4):\n",
    "    train_dataset = train_dataset.concatenate(dataset.shard(num_shards=5, index=i))\n",
    "val_dataset = dataset.shard(num_shards=5, index=4)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu và áp dụng prefetch\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1  # Trừ header\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(15, 3)),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision()]\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 10. Lưu mô hình sau khi training\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
