{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Load dữ liệu\n",
    "data = pd.read_csv('./train-data/FINANCIAL_DATA.csv')\n",
    "# Hàm tạo dataset dạng sliding window với target đã có sẵn (không dùng scaler)\n",
    "def create_multifeature_dataset(df):\n",
    "    X = df[columns].values\n",
    "    Y = df['target'].values\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "time_step = 1\n",
    "X, y = create_multifeature_dataset(data)\n",
    "\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Chia dữ liệu thành tập train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, len(columns))),\n",
    "    LSTM(300, return_sequences=True),\n",
    "    LSTM(300, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Dùng hàm kích hoạt sigmoid cho đầu ra nhị phân\n",
    "])\n",
    "\n",
    "# Compile model với loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Định nghĩa callback dừng sớm nếu val_accuracy không cải thiện\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "          batch_size=64, verbose=1, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "# Lưu model (không lưu scaler vì không sử dụng)\n",
    "model.save('./model/predict_model_XAUUSD_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Hàm tạo dataset dạng sliding window với target đã có sẵn (không dùng scaler)\n",
    "def create_multifeature_dataset(df):\n",
    "    X = df[columns].values\n",
    "    Y = df['target'].values\n",
    "    return X, Y\n",
    "\n",
    "chunksize = 200000\n",
    "\n",
    "# Generator đọc dữ liệu theo từng chunk\n",
    "def data_generator(file_path, chunksize=300000):\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        X_chunk, y_chunk = create_multifeature_dataset(chunk)\n",
    "        # Reshape dữ liệu cho LSTM: (samples, time_step, features)\n",
    "        X_chunk = X_chunk.reshape((X_chunk.shape[0], 1, X_chunk.shape[1]))\n",
    "        yield X_chunk, y_chunk\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "time_step = 1\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, len(columns))),\n",
    "    LSTM(400, return_sequences=True),\n",
    "    LSTM(200, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model với loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), \n",
    "            loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.Precision()]\n",
    ")\n",
    "\n",
    "# Định nghĩa callback dừng sớm nếu val_precision không cải thiện\n",
    "\n",
    "\n",
    "file_path = './FOREX.csv'\n",
    "# Huấn luyện mô hình theo từng chunk từ generator với validation\n",
    "for X_chunk, y_chunk in data_generator(file_path, chunksize=chunksize):\n",
    "    # Tách dữ liệu của mỗi chunk thành train và validation\n",
    "    early_stopping = EarlyStopping(monitor='val_precision', patience=8, restore_best_weights=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_chunk, y_chunk, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "              batch_size=64, epochs=50, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Sau khi training xong, lưu model cuối cùng\n",
    "model.save('./model/predict_model_XAUUSD_v1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 14292/207442\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19:15\u001b[0m 6ms/step - loss: 0.6739 - precision: 0.5795"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 89\u001b[0m\n\u001b[0;32m     82\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     83\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     85\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# 9. Huấn luyện mô hình\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m          \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 10. Sau khi training, lưu model\u001b[39;00m\n\u001b[0;32m     98\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./predict_model_XAUUSD_v2.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:316\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    314\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m--> 316\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:103\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    101\u001b[0m         callback\u001b[38;5;241m.\u001b[39mon_train_batch_begin(batch, logs\u001b[38;5;241m=\u001b[39mlogs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    104\u001b[0m     logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    # Lấy các cột features theo thứ tự mong muốn\n",
    "    features = [batch[col] for col in columns]  # mỗi tensor có shape (batch_size,)\n",
    "    X = tf.stack(features, axis=1)              # shape: (batch_size, num_features)\n",
    "    X = tf.expand_dims(X, axis=1)                 # reshape thành (batch_size, time_steps=1, num_features)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "dataset = raw_dataset.map(parse_csv)\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset   = val_dataset.batch(batch_size)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, len(columns))),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=8,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Đọc file CSV lớn; Dask tự động chia thành nhiều partition\n",
    "df = dd.read_csv('./train-data/FOREX.csv')\n",
    "\n",
    "# Xáo trộn toàn bộ dữ liệu (lấy mẫu ngẫu nhiên với frac=1)\n",
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Gom tất cả các partition thành 1 partition\n",
    "df_single = df_shuffled.repartition(npartitions=1)\n",
    "\n",
    "# Ghi ra file CSV duy nhất (tham số single_file=True yêu cầu Dask phiên bản hỗ trợ)\n",
    "df_single.to_csv('FOREX.csv', index=False, single_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train-data/FINANCIAL_DATA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m         num_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num_lines\n\u001b[1;32m---> 65\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[43mcount_csv_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Chia 80% train và 20% validation\u001b[39;00m\n\u001b[0;32m     67\u001b[0m num_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_samples \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m, in \u001b[0;36mcount_csv_rows\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_csv_rows\u001b[39m(file_path):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     62\u001b[0m         num_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m num_lines\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train-data/FINANCIAL_DATA.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "\n",
    "# Định nghĩa lớp TransformerBlock\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Tham số đọc CSV và huấn luyện\n",
    "csv_file = './train-data/FINANCIAL_DATA.csv'\n",
    "chunksize = 50000   # Số dòng mỗi chunk khi đọc file\n",
    "batch_size = 64     # Số mẫu mỗi batch huấn luyện\n",
    "time_step = 1       # Lưu ý: với Transformer bạn có thể cân nhắc dùng chuỗi dài hơn nếu có thể\n",
    "\n",
    "# Generator đọc CSV theo từng phần (chunk)\n",
    "def data_generator(file_path, chunksize):\n",
    "    # Đọc CSV theo từng chunk để tiết kiệm bộ nhớ\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Lấy các cột features và cột target\n",
    "        X_chunk = chunk[columns].values\n",
    "        y_chunk = chunk['target'].values\n",
    "        # Reshape X_chunk thành dạng (số mẫu, time_step, số feature)\n",
    "        X_chunk = X_chunk.reshape((X_chunk.shape[0], time_step, X_chunk.shape[1]))\n",
    "        # Yield từng mẫu một để tf.data có thể batch sau này\n",
    "        for i in range(len(X_chunk)):\n",
    "            yield X_chunk[i], y_chunk[i]\n",
    "\n",
    "# Hàm đếm số dòng trong file CSV (loại header)\n",
    "def count_csv_rows(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        num_lines = sum(1 for line in f) - 1\n",
    "    return num_lines\n",
    "\n",
    "num_samples = count_csv_rows(csv_file)\n",
    "# Chia 80% train và 20% validation\n",
    "num_train = int(num_samples * 0.8)\n",
    "num_val = num_samples - num_train\n",
    "\n",
    "# Tạo tf.data.Dataset từ generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(csv_file, chunksize),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=((time_step, len(columns)), ())\n",
    ")\n",
    "\n",
    "# Tách dataset thành train và validation dựa trên số dòng đã tính\n",
    "train_dataset = dataset.take(num_train)\n",
    "val_dataset = dataset.skip(num_train)\n",
    "\n",
    "# Batch và prefetch để tăng hiệu năng đọc dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Tham số cho mô hình Transformer\n",
    "embed_dim = 64       # Kích thước embedding\n",
    "num_heads = 4        # Số heads của MultiHeadAttention\n",
    "ff_dim = 128         # Số nút trong feed-forward network\n",
    "dropout_rate = 0.2\n",
    "num_transformer_blocks = 2  # Số lớp transformer\n",
    "\n",
    "# Xây dựng mô hình Transformer bằng Functional API\n",
    "inputs = Input(shape=(time_step, len(columns)))\n",
    "x = Dense(embed_dim)(inputs)\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model với loss và metric cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Callback dừng sớm nếu val_precision không cải thiện sau 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình sử dụng dataset được tạo từ generator\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Lưu model đã huấn luyện\n",
    "model.save('./model/predict_model_XAUUSD_transformer_v2.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
