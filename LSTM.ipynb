{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m23484/63655\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m26:35\u001b[0m 40ms/step - accuracy: 0.5703 - loss: 0.6778"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Huấn luyện mô hình\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Lưu model (không lưu scaler vì không sử dụng)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model/predict_model_XAUUSD_v4.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Load dữ liệu\n",
    "data = pd.read_csv('./train-data/OANDA_XAUUSD_Historical.csv')\n",
    "\n",
    "# Hàm tạo dataset dạng sliding window với target đã có sẵn (không dùng scaler)\n",
    "def create_multifeature_dataset(df):\n",
    "    steps = []\n",
    "    for lag in range(0, 15):\n",
    "        step = np.stack([\n",
    "            df[f'body_{lag}'].values,\n",
    "            df[f'shadow_top_{lag}'].values,\n",
    "            df[f'shadow_bottom_{lag}'].values\n",
    "        ], axis=1)  # Shape: (num_samples, 3)\n",
    "        steps.append(step)\n",
    "\n",
    "    # Stack các bước theo chiều time để có shape (num_samples, 15, 3)\n",
    "    X = np.stack(steps, axis=1)\n",
    "    y = df['target'].values\n",
    "    return X, y\n",
    "\n",
    "X, y = create_multifeature_dataset(data)\n",
    "\n",
    "# Không cần reshape vì X đã có shape (num_samples, 15, 3)\n",
    "# X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Chia dữ liệu thành tập train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "model = Sequential([\n",
    "    Input(shape=(15, 3)),\n",
    "    LSTM(400, return_sequences=True),\n",
    "    LSTM(200, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Dùng hàm kích hoạt sigmoid cho đầu ra nhị phân\n",
    "])\n",
    "\n",
    "# Compile model với loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Định nghĩa callback dừng sớm nếu val_precision không cải thiện\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    validation_data=(X_test, y_test), \n",
    "    batch_size=64, \n",
    "    verbose=1, \n",
    "    epochs=10, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Lưu model (không lưu scaler vì không sử dụng)\n",
    "model.save('./model/predict_model_XAUUSD_v4.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "os.environ['TF_DATA_DISABLE_CACHE_LOCKING'] = '1'\n",
    "cache_dir = \"D:/tmp/FOREX_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    # Lấy các cột features theo thứ tự mong muốn\n",
    "    features = [batch[col] for col in columns]  # mỗi tensor có shape (batch_size,)\n",
    "    X = tf.stack(features, axis=1)              # shape: (batch_size, num_features)\n",
    "    X = tf.expand_dims(X, axis=1)                 # reshape thành (batch_size, time_steps=1, num_features)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "# dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE).cache(cache_dir).prefetch(tf.data.AUTOTUNE)\n",
    "dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, len(columns))),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/OANDA_XAUUSD_Historical.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    # shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    steps = []\n",
    "    # Lặp qua các lag từ 0 đến 14 để tạo ra 15 time step\n",
    "    for lag in range(0, 15):\n",
    "        # Mỗi time step có 3 đặc trưng: body, shadow_top, shadow_bottom\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # Kết quả: shape (batch_size, 3)\n",
    "        steps.append(step)\n",
    "    # Stack các time step lại theo chiều time: kết quả shape (batch_size, 15, 3)\n",
    "    X = tf.stack(steps, axis=1)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "dataset = raw_dataset.map(parse_csv).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset   = val_dataset.batch(batch_size)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LST\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(15, 3)),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "os.environ['TF_DATA_DISABLE_CACHE_LOCKING'] = '1'\n",
    "cache_dir = \"D:/tmp/FOREX_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(15)\n",
    "columns = [f'{col}_{lag}' for col in ['body', 'shadow_top', 'shadow_bottom'] for lag in lags]\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    steps = []\n",
    "    for lag in range(15):\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # (batch_size, 3)\n",
    "        steps.append(step)\n",
    "    X = tf.stack(steps, axis=1)  # (batch_size, 15, 3)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Sử dụng parallel map\n",
    "try:\n",
    "    dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "                         .cache(cache_dir) \\\n",
    "                         .prefetch(tf.data.AUTOTUNE)\n",
    "    # Thử duyệt qua một vài phần tử để kích hoạt cache.\n",
    "    for _ in dataset.take(1):\n",
    "        pass\n",
    "except tf.errors.DataLossError as e:\n",
    "    print(\"Cache bị lỗi, xóa cache và tạo lại...\")\n",
    "    shutil.rmtree(cache_dir)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "                         .cache(cache_dir) \\\n",
    "                         .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Prefetch để giảm latency\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. Chuyển đổi thành từng mẫu đơn\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# 5. Chia dữ liệu sử dụng shard (thay cho enumerate+filter)\n",
    "# Giả sử chia thành 5 shard: 80% cho train (shard 0,1,2,3) và 20% cho validation (shard 4)\n",
    "# Phương án này đòi hỏi dữ liệu đã được xáo trộn\n",
    "train_shards = [dataset.shard(num_shards=5, index=i) for i in range(4)]\n",
    "train_dataset = train_shards[0]\n",
    "for shard in train_shards[1:]:\n",
    "    train_dataset = train_dataset.concatenate(shard)\n",
    "\n",
    "val_dataset = dataset.shard(num_shards=5, index=4)\n",
    "\n",
    "# 6. Re-batch, repeat và prefetch\n",
    "train_dataset = train_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 7. Tính số bước (nếu cần tính toán bước dựa trên số dòng)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(15, 3)),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Lưu model\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    # Lấy các cột features theo thứ tự mong muốn\n",
    "    features = [batch[col] for col in columns]  # mỗi tensor có shape (batch_size,)\n",
    "    X = tf.stack(features, axis=1)              # shape: (batch_size, num_features)\n",
    "    X = tf.expand_dims(X, axis=1)                 # reshape thành (batch_size, time_steps=1, num_features)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "dataset = raw_dataset.map(parse_csv)\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình MLP\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, len(columns))),\n",
    "    tf.keras.layers.Flatten(),  # chuyển đổi từ (batch_size, 1, num_features) -> (batch_size, num_features)\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Đọc file CSV lớn; Dask tự động chia thành nhiều partition\n",
    "df = dd.read_csv('./train-data/FOREX.csv')\n",
    "\n",
    "# Xáo trộn toàn bộ dữ liệu (lấy mẫu ngẫu nhiên với frac=1)\n",
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Gom tất cả các partition thành 1 partition\n",
    "df_single = df_shuffled.repartition(npartitions=1)\n",
    "\n",
    "# Ghi ra file CSV duy nhất (tham số single_file=True yêu cầu Dask phiên bản hỗ trợ)\n",
    "df_single.to_csv('FOREX.csv', index=False, single_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. Cài đặt tf.distribute để chạy song song trên GPU\n",
    "strategy = tf.distribute.MirroredStrategy()  # Sử dụng tất cả các GPU có sẵn\n",
    "\n",
    "# Đường dẫn đến file CSV và thiết lập batch size\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 2. Xây dựng danh sách các cột features với các lag\n",
    "lags = list(range(15))\n",
    "def get_columns_for_lag(lag):\n",
    "    return [f'body_{lag}', f'shadow_top_{lag}', f'shadow_bottom_{lag}']\n",
    "\n",
    "columns = []\n",
    "for lag in lags:\n",
    "    columns.extend(get_columns_for_lag(lag))\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 3. Tạo dataset streaming từ CSV (sử dụng CPU)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "def parse_csv(batch):\n",
    "    # Xây dựng dữ liệu với shape: (batch_size, time_steps=15, features=3)\n",
    "    X_steps = []\n",
    "    for lag in lags:\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # shape: (batch_size, 3)\n",
    "        X_steps.append(step)\n",
    "    X = tf.stack(X_steps, axis=1)  # shape: (batch_size, 15, 3)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Áp dụng map để chuyển đổi dữ liệu\n",
    "dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Tính số lượng batch theo số dòng (nếu cần)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "total_steps = total_samples // batch_size\n",
    "train_batches = int(0.8 * total_steps)\n",
    "val_batches = total_steps - train_batches\n",
    "\n",
    "# Phân chia tập train và validation bằng take và skip\n",
    "train_dataset = dataset.take(train_batches).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = dataset.skip(train_batches).take(val_batches).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. Xây dựng mô hình Transformer trong phạm vi của strategy\n",
    "with strategy.scope():\n",
    "    # Tham số mô hình\n",
    "    time_steps = len(lags)       # 15\n",
    "    num_features = 3             # mỗi time step có 3 features\n",
    "    embedding_dim = 64           # chiều embedding cho mỗi time step\n",
    "    num_transformer_blocks = 2   # số Transformer Encoder block\n",
    "    head_size = 64\n",
    "    num_heads = 4\n",
    "    ff_dim = 128\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    # Lớp Positional Embedding\n",
    "    class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "        def __init__(self, sequence_length, output_dim):\n",
    "            super(PositionalEmbedding, self).__init__()\n",
    "            self.token_proj = tf.keras.layers.Dense(output_dim)\n",
    "            self.position_embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=output_dim\n",
    "            )\n",
    "            self.sequence_length = sequence_length\n",
    "\n",
    "        def call(self, inputs):\n",
    "            positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "            pos_embed = self.position_embedding(positions)  # (sequence_length, output_dim)\n",
    "            token_embed = self.token_proj(inputs)           # (batch_size, sequence_length, output_dim)\n",
    "            return token_embed + pos_embed\n",
    "\n",
    "    # Định nghĩa Transformer Encoder block\n",
    "    def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout):\n",
    "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        attn_output = tf.keras.layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )(x, x)\n",
    "        x = tf.keras.layers.Add()([attn_output, inputs])\n",
    "        x_ff = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x_ff = tf.keras.layers.Dense(ff_dim, activation=\"relu\")(x_ff)\n",
    "        x_ff = tf.keras.layers.Dropout(dropout)(x_ff)\n",
    "        x_ff = tf.keras.layers.Dense(inputs.shape[-1])(x_ff)\n",
    "        return tf.keras.layers.Add()([x, x_ff])\n",
    "\n",
    "    # Xây dựng mô hình\n",
    "    inputs = tf.keras.layers.Input(shape=(time_steps, num_features))\n",
    "    x = PositionalEmbedding(sequence_length=time_steps, output_dim=embedding_dim)(inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_precision',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 5. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=1,\n",
    "          steps_per_epoch=train_batches,\n",
    "          validation_steps=val_batches,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 6. Lưu mô hình sau training\n",
    "model.save('./predict_model_XAUUSD_transformer_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = [f'{col}_{lag}' for col in ['body', 'shadow_top', 'shadow_bottom'] for lag in lags]\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV với đọc song song\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    num_parallel_reads=tf.data.experimental.AUTOTUNE\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    steps = []\n",
    "    for lag in range(0, 15):\n",
    "        step = tf.stack([\n",
    "            batch[f'body_{lag}'],\n",
    "            batch[f'shadow_top_{lag}'],\n",
    "            batch[f'shadow_bottom_{lag}']\n",
    "        ], axis=1)  # Kết quả shape: (batch_size, 3)\n",
    "        steps.append(step)\n",
    "    # Stack theo chiều thời gian: (batch_size, 15, 3)\n",
    "    X = tf.stack(steps, axis=1)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Áp dụng map với xử lý song song\n",
    "dataset = raw_dataset.map(parse_csv, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 4. Tùy chọn: Nếu dữ liệu đủ nhỏ để cache, có thể cache sau map\n",
    "# dataset = dataset.cache()\n",
    "\n",
    "# 5. Tách dữ liệu thành các mẫu riêng lẻ và sử dụng shard để chia train/validation\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# Sử dụng shard: Chia dữ liệu thành 5 phần, 4 phần train, 1 phần validation\n",
    "train_dataset = dataset.shard(num_shards=5, index=0)\n",
    "for i in range(1, 4):\n",
    "    train_dataset = train_dataset.concatenate(dataset.shard(num_shards=5, index=i))\n",
    "val_dataset = dataset.shard(num_shards=5, index=4)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu và áp dụng prefetch\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1  # Trừ header\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(15, 3)),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision()]\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 10. Lưu mô hình sau khi training\n",
    "model.save('./predict_model_XAUUSD_v3.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
