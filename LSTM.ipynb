{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Load dữ liệu\n",
    "data = pd.read_csv('./train-data/FINANCIAL_DATA.csv')\n",
    "# Hàm tạo dataset dạng sliding window với target đã có sẵn (không dùng scaler)\n",
    "def create_multifeature_dataset(df):\n",
    "    X = df[columns].values\n",
    "    Y = df['target'].values\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "time_step = 1\n",
    "X, y = create_multifeature_dataset(data)\n",
    "\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Chia dữ liệu thành tập train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, len(columns))),\n",
    "    LSTM(300, return_sequences=True),\n",
    "    LSTM(300, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Dùng hàm kích hoạt sigmoid cho đầu ra nhị phân\n",
    "])\n",
    "\n",
    "# Compile model với loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Định nghĩa callback dừng sớm nếu val_accuracy không cải thiện\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "          batch_size=64, verbose=1, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "# Lưu model (không lưu scaler vì không sử dụng)\n",
    "model.save('./model/predict_model_XAUUSD_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Đường dẫn file CSV và tham số\n",
    "csv_file = './train-data/FINANCIAL_DATA.csv'\n",
    "chunksize = 50000   # Số dòng mỗi chunk khi đọc file\n",
    "batch_size = 64     # Số mẫu mỗi batch huấn luyện\n",
    "\n",
    "# Hàm đếm số dòng trong file CSV (loại header)\n",
    "def count_csv_rows(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        num_lines = sum(1 for line in f) - 1\n",
    "    return num_lines\n",
    "\n",
    "num_samples = count_csv_rows(csv_file)\n",
    "# Giả sử ta chia 80% cho train và 20% cho validation\n",
    "num_train = int(num_samples * 0.8)\n",
    "num_val = num_samples - num_train\n",
    "\n",
    "# Định nghĩa generator sử dụng pandas.read_csv theo từng chunk\n",
    "def data_generator(file_path, chunksize):\n",
    "    # Đọc file CSV theo từng phần (chunk)\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Lấy các cột feature theo danh sách columns và cột target\n",
    "        X_chunk = chunk[columns].values\n",
    "        y_chunk = chunk['target'].values\n",
    "        # Reshape X_chunk thành dạng (số mẫu, time_step, số feature)\n",
    "        X_chunk = X_chunk.reshape((X_chunk.shape[0], 1, X_chunk.shape[1]))\n",
    "        # Yield từng mẫu để tf.data có thể batch lại sau\n",
    "        for i in range(len(X_chunk)):\n",
    "            yield X_chunk[i], y_chunk[i]\n",
    "\n",
    "# Tạo tf.data.Dataset từ generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(csv_file, chunksize),\n",
    "    output_types=(tf.float32, tf.float32),         # Giả sử target dưới dạng số thực (0/1)\n",
    "    output_shapes=((1, len(columns)), ())           # X có shape (1, số feature), y là scalar\n",
    ")\n",
    "\n",
    "# Tách dataset thành phần train và validation dựa trên số dòng đã tính\n",
    "train_dataset = dataset.take(num_train)\n",
    "val_dataset = dataset.skip(num_train)\n",
    "\n",
    "# Batch và prefetch để tăng hiệu năng đọc dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "time_step = 1\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, len(columns))),\n",
    "    LSTM(300, return_sequences=True),\n",
    "    LSTM(300, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model với optimizer Adam và loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Callback dừng sớm khi val_precision không cải thiện sau 5 epoch\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Xác định số bước mỗi epoch dựa trên số mẫu training\n",
    "steps_per_epoch = math.ceil(num_train / batch_size)\n",
    "validation_steps = math.ceil(num_val / batch_size)\n",
    "\n",
    "# Huấn luyện mô hình sử dụng dataset tạo từ generator\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Lưu mô hình đã huấn luyện\n",
    "model.save('./model/predict_model_XAUUSD_v3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "\n",
    "# Định nghĩa lớp TransformerBlock\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Tham số đọc CSV và huấn luyện\n",
    "csv_file = './train-data/FINANCIAL_DATA.csv'\n",
    "chunksize = 50000   # Số dòng mỗi chunk khi đọc file\n",
    "batch_size = 64     # Số mẫu mỗi batch huấn luyện\n",
    "time_step = 1       # Lưu ý: với Transformer bạn có thể cân nhắc dùng chuỗi dài hơn nếu có thể\n",
    "\n",
    "# Generator đọc CSV theo từng phần (chunk)\n",
    "def data_generator(file_path, chunksize):\n",
    "    # Đọc CSV theo từng chunk để tiết kiệm bộ nhớ\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Lấy các cột features và cột target\n",
    "        X_chunk = chunk[columns].values\n",
    "        y_chunk = chunk['target'].values\n",
    "        # Reshape X_chunk thành dạng (số mẫu, time_step, số feature)\n",
    "        X_chunk = X_chunk.reshape((X_chunk.shape[0], time_step, X_chunk.shape[1]))\n",
    "        # Yield từng mẫu một để tf.data có thể batch sau này\n",
    "        for i in range(len(X_chunk)):\n",
    "            yield X_chunk[i], y_chunk[i]\n",
    "\n",
    "# Hàm đếm số dòng trong file CSV (loại header)\n",
    "def count_csv_rows(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        num_lines = sum(1 for line in f) - 1\n",
    "    return num_lines\n",
    "\n",
    "num_samples = count_csv_rows(csv_file)\n",
    "# Chia 80% train và 20% validation\n",
    "num_train = int(num_samples * 0.8)\n",
    "num_val = num_samples - num_train\n",
    "\n",
    "# Tạo tf.data.Dataset từ generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(csv_file, chunksize),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=((time_step, len(columns)), ())\n",
    ")\n",
    "\n",
    "# Tách dataset thành train và validation dựa trên số dòng đã tính\n",
    "train_dataset = dataset.take(num_train)\n",
    "val_dataset = dataset.skip(num_train)\n",
    "\n",
    "# Batch và prefetch để tăng hiệu năng đọc dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Tham số cho mô hình Transformer\n",
    "embed_dim = 64       # Kích thước embedding\n",
    "num_heads = 4        # Số heads của MultiHeadAttention\n",
    "ff_dim = 128         # Số nút trong feed-forward network\n",
    "dropout_rate = 0.2\n",
    "num_transformer_blocks = 2  # Số lớp transformer\n",
    "\n",
    "# Xây dựng mô hình Transformer bằng Functional API\n",
    "inputs = Input(shape=(time_step, len(columns)))\n",
    "x = Dense(embed_dim)(inputs)\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model với loss và metric cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Callback dừng sớm nếu val_precision không cải thiện sau 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình sử dụng dataset được tạo từ generator\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Lưu model đã huấn luyện\n",
    "model.save('./model/predict_model_XAUUSD_transformer_v2.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
