{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Load dữ liệu\n",
    "data = pd.read_csv('./train-data/FINANCIAL_DATA.csv')\n",
    "# Hàm tạo dataset dạng sliding window với target đã có sẵn (không dùng scaler)\n",
    "def create_multifeature_dataset(df):\n",
    "    X = df[columns].values\n",
    "    Y = df['target'].values\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "time_step = 1\n",
    "X, y = create_multifeature_dataset(data)\n",
    "\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Chia dữ liệu thành tập train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, len(columns))),\n",
    "    LSTM(300, return_sequences=True),\n",
    "    LSTM(300, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Dùng hàm kích hoạt sigmoid cho đầu ra nhị phân\n",
    "])\n",
    "\n",
    "# Compile model với loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Định nghĩa callback dừng sớm nếu val_accuracy không cải thiện\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "          batch_size=64, verbose=1, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "# Lưu model (không lưu scaler vì không sử dụng)\n",
    "model.save('./model/predict_model_XAUUSD_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Hàm tạo dataset dạng sliding window với target đã có sẵn (không dùng scaler)\n",
    "def create_multifeature_dataset(df):\n",
    "    X = df[columns].values\n",
    "    Y = df['target'].values\n",
    "    return X, Y\n",
    "\n",
    "chunksize = 200000\n",
    "\n",
    "# Generator đọc dữ liệu theo từng chunk\n",
    "def data_generator(file_path, chunksize=300000):\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        X_chunk, y_chunk = create_multifeature_dataset(chunk)\n",
    "        # Reshape dữ liệu cho LSTM: (samples, time_step, features)\n",
    "        X_chunk = X_chunk.reshape((X_chunk.shape[0], 1, X_chunk.shape[1]))\n",
    "        yield X_chunk, y_chunk\n",
    "\n",
    "# Xây dựng mô hình LSTM cho bài toán phân loại nhị phân\n",
    "time_step = 1\n",
    "model = Sequential([\n",
    "    Input(shape=(time_step, len(columns))),\n",
    "    LSTM(400, return_sequences=True),\n",
    "    LSTM(200, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model với loss cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), \n",
    "            loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.Precision()]\n",
    ")\n",
    "\n",
    "# Định nghĩa callback dừng sớm nếu val_precision không cải thiện\n",
    "\n",
    "\n",
    "file_path = './data.csv'\n",
    "# Huấn luyện mô hình theo từng chunk từ generator với validation\n",
    "for X_chunk, y_chunk in data_generator(file_path, chunksize=chunksize):\n",
    "    # Tách dữ liệu của mỗi chunk thành train và validation\n",
    "    early_stopping = EarlyStopping(monitor='val_precision', patience=8, restore_best_weights=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_chunk, y_chunk, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "              batch_size=64, epochs=50, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Sau khi training xong, lưu model cuối cùng\n",
    "model.save('./model/predict_model_XAUUSD_v1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Đường dẫn đến file CSV\n",
    "file_path = './train-data/FOREX.csv'\n",
    "batch_size = 64\n",
    "\n",
    "# 1. Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "target_column = 'target'\n",
    "all_columns = columns + [target_column]\n",
    "\n",
    "# 2. Tạo dataset streaming từ CSV (không load toàn bộ vào bộ nhớ)\n",
    "raw_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_path,\n",
    "    batch_size=batch_size,\n",
    "    header=True,\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    ").ignore_errors()\n",
    "\n",
    "# 3. Hàm xử lý mỗi batch: sắp xếp lại thứ tự các cột và reshape cho LSTM\n",
    "def parse_csv(batch):\n",
    "    # Lấy các cột features theo thứ tự mong muốn\n",
    "    features = [batch[col] for col in columns]  # mỗi tensor có shape (batch_size,)\n",
    "    X = tf.stack(features, axis=1)              # shape: (batch_size, num_features)\n",
    "    X = tf.expand_dims(X, axis=1)                 # reshape thành (batch_size, time_steps=1, num_features)\n",
    "    y = batch[target_column]\n",
    "    return X, y\n",
    "\n",
    "dataset = raw_dataset.map(parse_csv)\n",
    "\n",
    "# 4. Unbatch và enumerate để xử lý từng mẫu riêng lẻ, từ đó chia theo tỷ lệ cố định\n",
    "dataset = dataset.unbatch()       # Tách ra thành từng mẫu đơn\n",
    "dataset = dataset.enumerate()     # Gán chỉ số cho mỗi mẫu: (index, (X, y))\n",
    "\n",
    "# 5. Chia dữ liệu theo modulo (ví dụ: 80% train, 20% validation)\n",
    "def is_train(idx, data):\n",
    "    # Với mỗi 5 mẫu, 4 mẫu đầu cho train (idx mod 5 < 4)\n",
    "    return tf.math.mod(idx, 5) < 4\n",
    "\n",
    "def is_val(idx, data):\n",
    "    return tf.math.mod(idx, 5) == 4\n",
    "\n",
    "train_dataset = dataset.filter(is_train).map(lambda idx, data: data)\n",
    "val_dataset   = dataset.filter(is_val).map(lambda idx, data: data)\n",
    "\n",
    "# 6. Re-batch lại dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset   = val_dataset.batch(batch_size)\n",
    "\n",
    "# (Tùy chọn) Nếu cần lặp lại dataset vô hạn:\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset   = val_dataset.repeat()\n",
    "\n",
    "# 7. Tính số bước (steps) dựa trên số dòng trong file CSV\n",
    "# Đếm số dòng trong file (trừ header)\n",
    "with open(file_path, 'r') as f:\n",
    "    total_samples = sum(1 for line in f) - 1\n",
    "\n",
    "# Vì ta chia theo tỉ lệ 80/20, ta tính số steps cho train và validation\n",
    "total_steps = total_samples // batch_size\n",
    "train_steps = int(0.8 * total_steps)\n",
    "validation_steps = int(0.2 * total_steps)\n",
    "\n",
    "# 8. Xây dựng mô hình LSTM\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, len(columns))),\n",
    "    tf.keras.layers.LSTM(400, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(200, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Sử dụng 'val_loss' để đảm bảo metric có giá trị ngay từ epoch đầu\n",
    "    patience=8,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 9. Huấn luyện mô hình\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_steps=validation_steps,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1)\n",
    "\n",
    "# 10. Sau khi training, lưu model\n",
    "model.save('./predict_model_XAUUSD_v2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Đọc file CSV lớn; Dask tự động chia thành nhiều partition\n",
    "df = dd.read_csv('./train-data/FOREX.csv')\n",
    "\n",
    "# Xáo trộn toàn bộ dữ liệu (lấy mẫu ngẫu nhiên với frac=1)\n",
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Gom tất cả các partition thành 1 partition\n",
    "df_single = df_shuffled.repartition(npartitions=1)\n",
    "\n",
    "# Ghi ra file CSV duy nhất (tham số single_file=True yêu cầu Dask phiên bản hỗ trợ)\n",
    "df_single.to_csv('FOREX.csv', index=False, single_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "\n",
    "# Định nghĩa lớp TransformerBlock\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Xây dựng danh sách các cột features với các lag\n",
    "lags = range(0, 15)\n",
    "columns = []\n",
    "for col in ['body', 'shadow_top', 'shadow_bottom']:\n",
    "    for lag in lags:\n",
    "        columns.append(f'{col}_{lag}')\n",
    "\n",
    "# Tham số đọc CSV và huấn luyện\n",
    "csv_file = './train-data/FINANCIAL_DATA.csv'\n",
    "chunksize = 50000   # Số dòng mỗi chunk khi đọc file\n",
    "batch_size = 64     # Số mẫu mỗi batch huấn luyện\n",
    "time_step = 1       # Lưu ý: với Transformer bạn có thể cân nhắc dùng chuỗi dài hơn nếu có thể\n",
    "\n",
    "# Generator đọc CSV theo từng phần (chunk)\n",
    "def data_generator(file_path, chunksize):\n",
    "    # Đọc CSV theo từng chunk để tiết kiệm bộ nhớ\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        # Lấy các cột features và cột target\n",
    "        X_chunk = chunk[columns].values\n",
    "        y_chunk = chunk['target'].values\n",
    "        # Reshape X_chunk thành dạng (số mẫu, time_step, số feature)\n",
    "        X_chunk = X_chunk.reshape((X_chunk.shape[0], time_step, X_chunk.shape[1]))\n",
    "        # Yield từng mẫu một để tf.data có thể batch sau này\n",
    "        for i in range(len(X_chunk)):\n",
    "            yield X_chunk[i], y_chunk[i]\n",
    "\n",
    "# Hàm đếm số dòng trong file CSV (loại header)\n",
    "def count_csv_rows(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        num_lines = sum(1 for line in f) - 1\n",
    "    return num_lines\n",
    "\n",
    "num_samples = count_csv_rows(csv_file)\n",
    "# Chia 80% train và 20% validation\n",
    "num_train = int(num_samples * 0.8)\n",
    "num_val = num_samples - num_train\n",
    "\n",
    "# Tạo tf.data.Dataset từ generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(csv_file, chunksize),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=((time_step, len(columns)), ())\n",
    ")\n",
    "\n",
    "# Tách dataset thành train và validation dựa trên số dòng đã tính\n",
    "train_dataset = dataset.take(num_train)\n",
    "val_dataset = dataset.skip(num_train)\n",
    "\n",
    "# Batch và prefetch để tăng hiệu năng đọc dữ liệu\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Tham số cho mô hình Transformer\n",
    "embed_dim = 64       # Kích thước embedding\n",
    "num_heads = 4        # Số heads của MultiHeadAttention\n",
    "ff_dim = 128         # Số nút trong feed-forward network\n",
    "dropout_rate = 0.2\n",
    "num_transformer_blocks = 2  # Số lớp transformer\n",
    "\n",
    "# Xây dựng mô hình Transformer bằng Functional API\n",
    "inputs = Input(shape=(time_step, len(columns)))\n",
    "x = Dense(embed_dim)(inputs)\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model với loss và metric cho phân loại nhị phân\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', tf.keras.metrics.Precision()])\n",
    "\n",
    "# Callback dừng sớm nếu val_precision không cải thiện sau 5 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Huấn luyện mô hình sử dụng dataset được tạo từ generator\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Lưu model đã huấn luyện\n",
    "model.save('./model/predict_model_XAUUSD_transformer_v2.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
